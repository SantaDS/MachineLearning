{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XLNet_Baseline.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SantaDS/MachineLearning/blob/master/XLNet_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbKKFHC8KoIT",
        "colab_type": "code",
        "outputId": "a7bad224-1398-484b-82de-9349e819e32a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 5.0MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oljkdC2kJLKl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from absl import flags\n",
        "import absl.logging as _logging  # pylint: disable=unused-import\n",
        "\n",
        "import collections\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import json\n",
        "import six\n",
        "import random\n",
        "import gc\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "if six.PY2:\n",
        "  import cPickle as pickle\n",
        "else:\n",
        "  import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "import sentencepiece as spm\n",
        "#from prepro_utils import preprocess_text, encode_ids, encode_pieces, printable_text\n",
        "#import function_builder\n",
        "#import model_utils\n",
        "#import squad_utils\n",
        "#from data_utils import SEP_ID, CLS_ID, VOCAB_SIZE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Arz3eIMgzVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "  \"\"\"Gaussian Error Linear Unit.\n",
        "  This is a smoother version of the RELU.\n",
        "  Original paper: https://arxiv.org/abs/1606.08415\n",
        "  Args:\n",
        "    x: float Tensor to perform activation.\n",
        "  Returns:\n",
        "    `x` with the GELU activation applied.\n",
        "  \"\"\"\n",
        "  cdf = 0.5 * (1.0 + tf.tanh(\n",
        "      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
        "  return x * cdf\n",
        "\n",
        "\n",
        "def embedding_lookup(x, n_token, d_embed, initializer, use_tpu=True,\n",
        "                     scope='embedding', reuse=None, dtype=tf.float32):\n",
        "  \"\"\"TPU and GPU embedding_lookup function.\"\"\"\n",
        "  with tf.variable_scope(scope, reuse=reuse):\n",
        "    lookup_table = tf.get_variable('lookup_table', [n_token, d_embed],\n",
        "                                   dtype=dtype, initializer=initializer)\n",
        "    if use_tpu:\n",
        "      one_hot_idx = tf.one_hot(x, n_token, dtype=dtype)\n",
        "      if one_hot_idx.shape.ndims == 2:\n",
        "        return tf.einsum('in,nd->id', one_hot_idx, lookup_table), lookup_table\n",
        "      else:\n",
        "        return tf.einsum('ibn,nd->ibd', one_hot_idx, lookup_table), lookup_table\n",
        "    else:\n",
        "      return tf.nn.embedding_lookup(lookup_table, x), lookup_table\n",
        "\n",
        "\n",
        "def positional_embedding(pos_seq, inv_freq, bsz=None):\n",
        "  sinusoid_inp = tf.einsum('i,d->id', pos_seq, inv_freq)\n",
        "  pos_emb = tf.concat([tf.sin(sinusoid_inp), tf.cos(sinusoid_inp)], -1)\n",
        "  pos_emb = pos_emb[:, None, :]\n",
        "\n",
        "  if bsz is not None:\n",
        "    pos_emb = tf.tile(pos_emb, [1, bsz, 1])\n",
        "\n",
        "  return pos_emb\n",
        "\n",
        "\n",
        "def positionwise_ffn(inp, d_model, d_inner, dropout, kernel_initializer,\n",
        "                     activation_type='relu', scope='ff', is_training=True,\n",
        "                     reuse=None):\n",
        "  \"\"\"Position-wise Feed-forward Network.\"\"\"\n",
        "  if activation_type == 'relu':\n",
        "    activation = tf.nn.relu\n",
        "  elif activation_type == 'gelu':\n",
        "    activation = gelu\n",
        "  else:\n",
        "    raise ValueError('Unsupported activation type {}'.format(activation_type))\n",
        "\n",
        "  output = inp\n",
        "  with tf.variable_scope(scope, reuse=reuse):\n",
        "    output = tf.layers.dense(output, d_inner, activation=activation,\n",
        "                             kernel_initializer=kernel_initializer,\n",
        "                             name='layer_1')\n",
        "    output = tf.layers.dropout(output, dropout, training=is_training,\n",
        "                               name='drop_1')\n",
        "    output = tf.layers.dense(output, d_model,\n",
        "                             kernel_initializer=kernel_initializer,\n",
        "                             name='layer_2')\n",
        "    output = tf.layers.dropout(output, dropout, training=is_training,\n",
        "                               name='drop_2')\n",
        "    output = tf.contrib.layers.layer_norm(output + inp, begin_norm_axis=-1,\n",
        "                                          scope='LayerNorm')\n",
        "  return output\n",
        "\n",
        "\n",
        "def head_projection(h, d_model, n_head, d_head, kernel_initializer, name):\n",
        "  \"\"\"Project hidden states to a specific head with a 4D-shape.\"\"\"\n",
        "  proj_weight = tf.get_variable('{}/kernel'.format(name),\n",
        "                                [d_model, n_head, d_head], dtype=h.dtype,\n",
        "                                initializer=kernel_initializer)\n",
        "  head = tf.einsum('ibh,hnd->ibnd', h, proj_weight)\n",
        "\n",
        "  return head\n",
        "\n",
        "\n",
        "def post_attention(h, attn_vec, d_model, n_head, d_head, dropout, is_training,\n",
        "                   kernel_initializer, residual=True):\n",
        "  \"\"\"Post-attention processing.\"\"\"\n",
        "  # post-attention projection (back to `d_model`)\n",
        "  proj_o = tf.get_variable('o/kernel', [d_model, n_head, d_head],\n",
        "                           dtype=h.dtype, initializer=kernel_initializer)\n",
        "  attn_out = tf.einsum('ibnd,hnd->ibh', attn_vec, proj_o)\n",
        "\n",
        "  attn_out = tf.layers.dropout(attn_out, dropout, training=is_training)\n",
        "  if residual:\n",
        "    output = tf.contrib.layers.layer_norm(attn_out + h, begin_norm_axis=-1,\n",
        "                                          scope='LayerNorm')\n",
        "  else:\n",
        "    output = tf.contrib.layers.layer_norm(attn_out, begin_norm_axis=-1,\n",
        "                                          scope='LayerNorm')\n",
        "\n",
        "  return output\n",
        "\n",
        "\n",
        "def abs_attn_core(q_head, k_head, v_head, attn_mask, dropatt, is_training,\n",
        "                  scale):\n",
        "  \"\"\"Core absolute positional attention operations.\"\"\"\n",
        "\n",
        "  attn_score = tf.einsum('ibnd,jbnd->ijbn', q_head, k_head)\n",
        "  attn_score *= scale\n",
        "  if attn_mask is not None:\n",
        "    attn_score = attn_score - 1e30 * attn_mask\n",
        "\n",
        "  # attention probability\n",
        "  attn_prob = tf.nn.softmax(attn_score, 1)\n",
        "  attn_prob = tf.layers.dropout(attn_prob, dropatt, training=is_training)\n",
        "\n",
        "  # attention output\n",
        "  attn_vec = tf.einsum('ijbn,jbnd->ibnd', attn_prob, v_head)\n",
        "\n",
        "  return attn_vec\n",
        "\n",
        "\n",
        "def rel_attn_core(q_head, k_head_h, v_head_h, k_head_r, seg_embed, seg_mat,\n",
        "                  r_w_bias, r_r_bias, r_s_bias, attn_mask, dropatt, is_training,\n",
        "                  scale):\n",
        "  \"\"\"Core relative positional attention operations.\"\"\"\n",
        "\n",
        "  # content based attention score\n",
        "  ac = tf.einsum('ibnd,jbnd->ijbn', q_head + r_w_bias, k_head_h)\n",
        "\n",
        "  # position based attention score\n",
        "  bd = tf.einsum('ibnd,jbnd->ijbn', q_head + r_r_bias, k_head_r)\n",
        "  bd = rel_shift(bd, klen=tf.shape(ac)[1])\n",
        "\n",
        "  # segment based attention score\n",
        "  if seg_mat is None:\n",
        "    ef = 0\n",
        "  else:\n",
        "    ef = tf.einsum('ibnd,snd->ibns', q_head + r_s_bias, seg_embed)\n",
        "    ef = tf.einsum('ijbs,ibns->ijbn', seg_mat, ef)\n",
        "\n",
        "  # merge attention scores and perform masking\n",
        "  attn_score = (ac + bd + ef) * scale\n",
        "  if attn_mask is not None:\n",
        "    # attn_score = attn_score * (1 - attn_mask) - 1e30 * attn_mask\n",
        "    attn_score = attn_score - 1e30 * attn_mask\n",
        "\n",
        "  # attention probability\n",
        "  attn_prob = tf.nn.softmax(attn_score, 1)\n",
        "  attn_prob = tf.layers.dropout(attn_prob, dropatt, training=is_training)\n",
        "\n",
        "  # attention output\n",
        "  attn_vec = tf.einsum('ijbn,jbnd->ibnd', attn_prob, v_head_h)\n",
        "\n",
        "  return attn_vec\n",
        "\n",
        "\n",
        "def rel_shift(x, klen=-1):\n",
        "  \"\"\"perform relative shift to form the relative attention score.\"\"\"\n",
        "  x_size = tf.shape(x)\n",
        "\n",
        "  x = tf.reshape(x, [x_size[1], x_size[0], x_size[2], x_size[3]])\n",
        "  x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n",
        "  x = tf.reshape(x, [x_size[0], x_size[1] - 1, x_size[2], x_size[3]])\n",
        "  x = tf.slice(x, [0, 0, 0, 0], [-1, klen, -1, -1])\n",
        "\n",
        "  return x\n",
        "\n",
        "\n",
        "def _create_mask(qlen, mlen, dtype=tf.float32, same_length=False):\n",
        "  \"\"\"create causal attention mask.\"\"\"\n",
        "  attn_mask = tf.ones([qlen, qlen], dtype=dtype)\n",
        "  mask_u = tf.matrix_band_part(attn_mask, 0, -1)\n",
        "  mask_dia = tf.matrix_band_part(attn_mask, 0, 0)\n",
        "  attn_mask_pad = tf.zeros([qlen, mlen], dtype=dtype)\n",
        "  ret = tf.concat([attn_mask_pad, mask_u - mask_dia], 1)\n",
        "  if same_length:\n",
        "    mask_l = tf.matrix_band_part(attn_mask, -1, 0)\n",
        "    ret = tf.concat([ret[:, :qlen] + mask_l - mask_dia, ret[:, qlen:]], 1)\n",
        "\n",
        "  return ret\n",
        "\n",
        "\n",
        "def _cache_mem(curr_out, prev_mem, mem_len, reuse_len=None):\n",
        "  \"\"\"cache hidden states into memory.\"\"\"\n",
        "  if mem_len is None or mem_len == 0:\n",
        "    return None\n",
        "  else:\n",
        "    if reuse_len is not None and reuse_len > 0:\n",
        "      curr_out = curr_out[:reuse_len]\n",
        "\n",
        "    if prev_mem is None:\n",
        "      new_mem = curr_out[-mem_len:]\n",
        "    else:\n",
        "      new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:]\n",
        "\n",
        "  return tf.stop_gradient(new_mem)\n",
        "\n",
        "\n",
        "def relative_positional_encoding(qlen, klen, d_model, clamp_len, attn_type,\n",
        "                                 bi_data, bsz=None, dtype=None):\n",
        "  \"\"\"create relative positional encoding.\"\"\"\n",
        "  freq_seq = tf.range(0, d_model, 2.0)\n",
        "  if dtype is not None and dtype != tf.float32:\n",
        "    freq_seq = tf.cast(freq_seq, dtype=dtype)\n",
        "  inv_freq = 1 / (10000 ** (freq_seq / d_model))\n",
        "\n",
        "  if attn_type == 'bi':\n",
        "    # beg, end = klen - 1, -qlen\n",
        "    beg, end = klen, -qlen\n",
        "  elif attn_type == 'uni':\n",
        "    # beg, end = klen - 1, -1\n",
        "    beg, end = klen, -1\n",
        "  else:\n",
        "    raise ValueError('Unknown `attn_type` {}.'.format(attn_type))\n",
        "\n",
        "  if bi_data:\n",
        "    fwd_pos_seq = tf.range(beg, end, -1.0)\n",
        "    bwd_pos_seq = tf.range(-beg, -end, 1.0)\n",
        "\n",
        "    if dtype is not None and dtype != tf.float32:\n",
        "      fwd_pos_seq = tf.cast(fwd_pos_seq, dtype=dtype)\n",
        "      bwd_pos_seq = tf.cast(bwd_pos_seq, dtype=dtype)\n",
        "\n",
        "    if clamp_len > 0:\n",
        "      fwd_pos_seq = tf.clip_by_value(fwd_pos_seq, -clamp_len, clamp_len)\n",
        "      bwd_pos_seq = tf.clip_by_value(bwd_pos_seq, -clamp_len, clamp_len)\n",
        "\n",
        "    if bsz is not None:\n",
        "      # With bi_data, the batch size should be divisible by 2.\n",
        "      assert bsz%2 == 0\n",
        "      fwd_pos_emb = positional_embedding(fwd_pos_seq, inv_freq, bsz//2)\n",
        "      bwd_pos_emb = positional_embedding(bwd_pos_seq, inv_freq, bsz//2)\n",
        "    else:\n",
        "      fwd_pos_emb = positional_embedding(fwd_pos_seq, inv_freq)\n",
        "      bwd_pos_emb = positional_embedding(bwd_pos_seq, inv_freq)\n",
        "\n",
        "    pos_emb = tf.concat([fwd_pos_emb, bwd_pos_emb], axis=1)\n",
        "  else:\n",
        "    fwd_pos_seq = tf.range(beg, end, -1.0)\n",
        "    if dtype is not None and dtype != tf.float32:\n",
        "      fwd_pos_seq = tf.cast(fwd_pos_seq, dtype=dtype)\n",
        "    if clamp_len > 0:\n",
        "      fwd_pos_seq = tf.clip_by_value(fwd_pos_seq, -clamp_len, clamp_len)\n",
        "    pos_emb = positional_embedding(fwd_pos_seq, inv_freq, bsz)\n",
        "\n",
        "  return pos_emb\n",
        "\n",
        "\n",
        "def multihead_attn(q, k, v, attn_mask, d_model, n_head, d_head, dropout,\n",
        "                   dropatt, is_training, kernel_initializer, residual=True,\n",
        "                   scope='abs_attn', reuse=None):\n",
        "  \"\"\"Standard multi-head attention with absolute positional embedding.\"\"\"\n",
        "\n",
        "  scale = 1 / (d_head ** 0.5)\n",
        "  with tf.variable_scope(scope, reuse=reuse):\n",
        "    # attention heads\n",
        "    q_head = head_projection(\n",
        "        q, d_model, n_head, d_head, kernel_initializer, 'q')\n",
        "    k_head = head_projection(\n",
        "        k, d_model, n_head, d_head, kernel_initializer, 'k')\n",
        "    v_head = head_projection(\n",
        "        v, d_model, n_head, d_head, kernel_initializer, 'v')\n",
        "\n",
        "    # attention vector\n",
        "    attn_vec = abs_attn_core(q_head, k_head, v_head, attn_mask, dropatt,\n",
        "                             is_training, scale)\n",
        "\n",
        "    # post processing\n",
        "    output = post_attention(v, attn_vec, d_model, n_head, d_head, dropout,\n",
        "                            is_training, kernel_initializer, residual)\n",
        "\n",
        "  return output\n",
        "\n",
        "\n",
        "\n",
        "def rel_multihead_attn(h, r, r_w_bias, r_r_bias, seg_mat, r_s_bias, seg_embed,\n",
        "                       attn_mask, mems, d_model, n_head, d_head, dropout,\n",
        "                       dropatt, is_training, kernel_initializer,\n",
        "                       scope='rel_attn', reuse=None):\n",
        "  \"\"\"Multi-head attention with relative positional encoding.\"\"\"\n",
        "\n",
        "  scale = 1 / (d_head ** 0.5)\n",
        "  with tf.variable_scope(scope, reuse=reuse):\n",
        "    if mems is not None and mems.shape.ndims > 1:\n",
        "      cat = tf.concat([mems, h], 0)\n",
        "    else:\n",
        "      cat = h\n",
        "\n",
        "    # content heads\n",
        "    q_head_h = head_projection(\n",
        "        h, d_model, n_head, d_head, kernel_initializer, 'q')\n",
        "    k_head_h = head_projection(\n",
        "        cat, d_model, n_head, d_head, kernel_initializer, 'k')\n",
        "    v_head_h = head_projection(\n",
        "        cat, d_model, n_head, d_head, kernel_initializer, 'v')\n",
        "\n",
        "    # positional heads\n",
        "    k_head_r = head_projection(\n",
        "        r, d_model, n_head, d_head, kernel_initializer, 'r')\n",
        "\n",
        "    # core attention ops\n",
        "    attn_vec = rel_attn_core(\n",
        "        q_head_h, k_head_h, v_head_h, k_head_r, seg_embed, seg_mat, r_w_bias,\n",
        "        r_r_bias, r_s_bias, attn_mask, dropatt, is_training, scale)\n",
        "\n",
        "    # post processing\n",
        "    output = post_attention(h, attn_vec, d_model, n_head, d_head, dropout,\n",
        "                            is_training, kernel_initializer)\n",
        "\n",
        "  return output\n",
        "\n",
        "\n",
        "def two_stream_rel_attn(h, g, r, mems, r_w_bias, r_r_bias, seg_mat, r_s_bias,\n",
        "                        seg_embed, attn_mask_h, attn_mask_g, target_mapping,\n",
        "                        d_model, n_head, d_head, dropout, dropatt, is_training,\n",
        "                        kernel_initializer, scope='rel_attn'):\n",
        "  \"\"\"Two-stream attention with relative positional encoding.\"\"\"\n",
        "\n",
        "  scale = 1 / (d_head ** 0.5)\n",
        "  with tf.variable_scope(scope, reuse=False):\n",
        "\n",
        "    # content based attention score\n",
        "    if mems is not None and mems.shape.ndims > 1:\n",
        "      cat = tf.concat([mems, h], 0)\n",
        "    else:\n",
        "      cat = h\n",
        "\n",
        "    # content-based key head\n",
        "    k_head_h = head_projection(\n",
        "        cat, d_model, n_head, d_head, kernel_initializer, 'k')\n",
        "\n",
        "    # content-based value head\n",
        "    v_head_h = head_projection(\n",
        "        cat, d_model, n_head, d_head, kernel_initializer, 'v')\n",
        "\n",
        "    # position-based key head\n",
        "    k_head_r = head_projection(\n",
        "        r, d_model, n_head, d_head, kernel_initializer, 'r')\n",
        "\n",
        "    ##### h-stream\n",
        "    # content-stream query head\n",
        "    q_head_h = head_projection(\n",
        "        h, d_model, n_head, d_head, kernel_initializer, 'q')\n",
        "\n",
        "    # core attention ops\n",
        "    attn_vec_h = rel_attn_core(\n",
        "        q_head_h, k_head_h, v_head_h, k_head_r, seg_embed, seg_mat, r_w_bias,\n",
        "        r_r_bias, r_s_bias, attn_mask_h, dropatt, is_training, scale)\n",
        "\n",
        "    # post processing\n",
        "    output_h = post_attention(h, attn_vec_h, d_model, n_head, d_head, dropout,\n",
        "                              is_training, kernel_initializer)\n",
        "\n",
        "  with tf.variable_scope(scope, reuse=True):\n",
        "    ##### g-stream\n",
        "    # query-stream query head\n",
        "    q_head_g = head_projection(\n",
        "        g, d_model, n_head, d_head, kernel_initializer, 'q')\n",
        "\n",
        "    # core attention ops\n",
        "    if target_mapping is not None:\n",
        "      q_head_g = tf.einsum('mbnd,mlb->lbnd', q_head_g, target_mapping)\n",
        "      attn_vec_g = rel_attn_core(\n",
        "          q_head_g, k_head_h, v_head_h, k_head_r, seg_embed, seg_mat, r_w_bias,\n",
        "          r_r_bias, r_s_bias, attn_mask_g, dropatt, is_training, scale)\n",
        "      attn_vec_g = tf.einsum('lbnd,mlb->mbnd', attn_vec_g, target_mapping)\n",
        "    else:\n",
        "      attn_vec_g = rel_attn_core(\n",
        "          q_head_g, k_head_h, v_head_h, k_head_r, seg_embed, seg_mat, r_w_bias,\n",
        "          r_r_bias, r_s_bias, attn_mask_g, dropatt, is_training, scale)\n",
        "\n",
        "    # post processing\n",
        "    output_g = post_attention(g, attn_vec_g, d_model, n_head, d_head, dropout,\n",
        "                              is_training, kernel_initializer)\n",
        "\n",
        "    return output_h, output_g\n",
        "\n",
        "\n",
        "def transformer_xl(inp_k, n_token, n_layer, d_model, n_head,\n",
        "                d_head, d_inner, dropout, dropatt, attn_type,\n",
        "                bi_data, initializer, is_training, mem_len=None,\n",
        "                inp_q=None, mems=None,\n",
        "                same_length=False, clamp_len=-1, untie_r=False,\n",
        "                use_tpu=True, input_mask=None,\n",
        "                perm_mask=None, seg_id=None, reuse_len=None,\n",
        "                ff_activation='relu', target_mapping=None,\n",
        "                use_bfloat16=False, scope='transformer', **kwargs):\n",
        "  \"\"\"\n",
        "    Defines a Transformer-XL computation graph with additional\n",
        "    support for XLNet.\n",
        "    Args:\n",
        "    inp_k: int32 Tensor in shape [len, bsz], the input token IDs.\n",
        "    seg_id: int32 Tensor in shape [len, bsz], the input segment IDs.\n",
        "    input_mask: float32 Tensor in shape [len, bsz], the input mask.\n",
        "      0 for real tokens and 1 for padding.\n",
        "    mems: a list of float32 Tensors in shape [mem_len, bsz, d_model], memory\n",
        "      from previous batches. The length of the list equals n_layer.\n",
        "      If None, no memory is used.\n",
        "    perm_mask: float32 Tensor in shape [len, len, bsz].\n",
        "      If perm_mask[i, j, k] = 0, i attend to j in batch k;\n",
        "      if perm_mask[i, j, k] = 1, i does not attend to j in batch k.\n",
        "      If None, each position attends to all the others.\n",
        "    target_mapping: float32 Tensor in shape [num_predict, len, bsz].\n",
        "      If target_mapping[i, j, k] = 1, the i-th predict in batch k is\n",
        "      on the j-th token.\n",
        "      Only used during pretraining for partial prediction.\n",
        "      Set to None during finetuning.\n",
        "    inp_q: float32 Tensor in shape [len, bsz].\n",
        "      1 for tokens with losses and 0 for tokens without losses.\n",
        "      Only used during pretraining for two-stream attention.\n",
        "      Set to None during finetuning.\n",
        "    n_layer: int, the number of layers.\n",
        "    d_model: int, the hidden size.\n",
        "    n_head: int, the number of attention heads.\n",
        "    d_head: int, the dimension size of each attention head.\n",
        "    d_inner: int, the hidden size in feed-forward layers.\n",
        "    ff_activation: str, \"relu\" or \"gelu\".\n",
        "    untie_r: bool, whether to untie the biases in attention.\n",
        "    n_token: int, the vocab size.\n",
        "    is_training: bool, whether in training mode.\n",
        "    use_tpu: bool, whether TPUs are used.\n",
        "    use_bfloat16: bool, use bfloat16 instead of float32.\n",
        "    dropout: float, dropout rate.\n",
        "    dropatt: float, dropout rate on attention probabilities.\n",
        "    init: str, the initialization scheme, either \"normal\" or \"uniform\".\n",
        "    init_range: float, initialize the parameters with a uniform distribution\n",
        "      in [-init_range, init_range]. Only effective when init=\"uniform\".\n",
        "    init_std: float, initialize the parameters with a normal distribution\n",
        "      with mean 0 and stddev init_std. Only effective when init=\"normal\".\n",
        "    mem_len: int, the number of tokens to cache.\n",
        "    reuse_len: int, the number of tokens in the currect batch to be cached\n",
        "      and reused in the future.\n",
        "    bi_data: bool, whether to use bidirectional input pipeline.\n",
        "      Usually set to True during pretraining and False during finetuning.\n",
        "    clamp_len: int, clamp all relative distances larger than clamp_len.\n",
        "      -1 means no clamping.\n",
        "    same_length: bool, whether to use the same attention length for each token.\n",
        "    summary_type: str, \"last\", \"first\", \"mean\", or \"attn\". The method\n",
        "      to pool the input to get a vector representation.\n",
        "    initializer: A tf initializer.\n",
        "    scope: scope name for the computation graph.\n",
        "  \"\"\"\n",
        "  tf.logging.info('memory input {}'.format(mems))\n",
        "  tf_float = tf.bfloat16 if use_bfloat16 else tf.float32\n",
        "  tf.logging.info('Use float type {}'.format(tf_float))\n",
        "\n",
        "  new_mems = []\n",
        "  with tf.variable_scope(scope):\n",
        "    if untie_r:\n",
        "      r_w_bias = tf.get_variable('r_w_bias', [n_layer, n_head, d_head],\n",
        "                                 dtype=tf_float, initializer=initializer)\n",
        "      r_r_bias = tf.get_variable('r_r_bias', [n_layer, n_head, d_head],\n",
        "                                 dtype=tf_float, initializer=initializer)\n",
        "    else:\n",
        "      r_w_bias = tf.get_variable('r_w_bias', [n_head, d_head],\n",
        "                                 dtype=tf_float, initializer=initializer)\n",
        "      r_r_bias = tf.get_variable('r_r_bias', [n_head, d_head],\n",
        "                                 dtype=tf_float, initializer=initializer)\n",
        "\n",
        "    bsz = tf.shape(inp_k)[1]\n",
        "    qlen = tf.shape(inp_k)[0]\n",
        "    mlen = tf.shape(mems[0])[0] if mems is not None else 0\n",
        "    klen = mlen + qlen\n",
        "\n",
        "    ##### Attention mask\n",
        "    # causal attention mask\n",
        "    if attn_type == 'uni':\n",
        "      attn_mask = _create_mask(qlen, mlen, tf_float, same_length)\n",
        "      attn_mask = attn_mask[:, :, None, None]\n",
        "    elif attn_type == 'bi':\n",
        "      attn_mask = None\n",
        "    else:\n",
        "      raise ValueError('Unsupported attention type: {}'.format(attn_type))\n",
        "\n",
        "    # data mask: input mask & perm mask\n",
        "    if input_mask is not None and perm_mask is not None:\n",
        "      data_mask = input_mask[None] + perm_mask\n",
        "    elif input_mask is not None and perm_mask is None:\n",
        "      data_mask = input_mask[None]\n",
        "    elif input_mask is None and perm_mask is not None:\n",
        "      data_mask = perm_mask\n",
        "    else:\n",
        "      data_mask = None\n",
        "\n",
        "    if data_mask is not None:\n",
        "      # all mems can be attended to\n",
        "      mems_mask = tf.zeros([tf.shape(data_mask)[0], mlen, bsz],\n",
        "                           dtype=tf_float)\n",
        "      data_mask = tf.concat([mems_mask, data_mask], 1)\n",
        "      if attn_mask is None:\n",
        "        attn_mask = data_mask[:, :, :, None]\n",
        "      else:\n",
        "        attn_mask += data_mask[:, :, :, None]\n",
        "\n",
        "    if attn_mask is not None:\n",
        "      attn_mask = tf.cast(attn_mask > 0, dtype=tf_float)\n",
        "\n",
        "    if attn_mask is not None:\n",
        "      non_tgt_mask = -tf.eye(qlen, dtype=tf_float)\n",
        "      non_tgt_mask = tf.concat([tf.zeros([qlen, mlen], dtype=tf_float),\n",
        "                                non_tgt_mask], axis=-1)\n",
        "      non_tgt_mask = tf.cast((attn_mask + non_tgt_mask[:, :, None, None]) > 0,\n",
        "                             dtype=tf_float)\n",
        "    else:\n",
        "      non_tgt_mask = None\n",
        "\n",
        "    ##### Word embedding\n",
        "    word_emb_k, lookup_table = embedding_lookup(\n",
        "        x=inp_k,\n",
        "        n_token=n_token,\n",
        "        d_embed=d_model,\n",
        "        initializer=initializer,\n",
        "        use_tpu=use_tpu,\n",
        "        dtype=tf_float,\n",
        "        scope='word_embedding')\n",
        "\n",
        "    if inp_q is not None:\n",
        "      with tf.variable_scope('mask_emb'):\n",
        "        mask_emb = tf.get_variable('mask_emb', [1, 1, d_model], dtype=tf_float)\n",
        "        if target_mapping is not None:\n",
        "          word_emb_q = tf.tile(mask_emb, [tf.shape(target_mapping)[0], bsz, 1])\n",
        "        else:\n",
        "          inp_q_ext = inp_q[:, :, None]\n",
        "          word_emb_q = inp_q_ext * mask_emb + (1 - inp_q_ext) * word_emb_k\n",
        "    output_h = tf.layers.dropout(word_emb_k, dropout, training=is_training)\n",
        "    if inp_q is not None:\n",
        "      output_g = tf.layers.dropout(word_emb_q, dropout, training=is_training)\n",
        "\n",
        "    ##### Segment embedding\n",
        "    if seg_id is not None:\n",
        "      if untie_r:\n",
        "        r_s_bias = tf.get_variable('r_s_bias', [n_layer, n_head, d_head],\n",
        "                                   dtype=tf_float, initializer=initializer)\n",
        "      else:\n",
        "        # default case (tie)\n",
        "        r_s_bias = tf.get_variable('r_s_bias', [n_head, d_head],\n",
        "                                   dtype=tf_float, initializer=initializer)\n",
        "\n",
        "      seg_embed = tf.get_variable('seg_embed', [n_layer, 2, n_head, d_head],\n",
        "                                  dtype=tf_float, initializer=initializer)\n",
        "\n",
        "      # Convert `seg_id` to one-hot `seg_mat`\n",
        "      mem_pad = tf.zeros([mlen, bsz], dtype=tf.int32)\n",
        "      cat_ids = tf.concat([mem_pad, seg_id], 0)\n",
        "\n",
        "      # `1` indicates not in the same segment [qlen x klen x bsz]\n",
        "      seg_mat = tf.cast(\n",
        "          tf.logical_not(tf.equal(seg_id[:, None], cat_ids[None, :])),\n",
        "          tf.int32)\n",
        "      seg_mat = tf.one_hot(seg_mat, 2, dtype=tf_float)\n",
        "    else:\n",
        "      seg_mat = None\n",
        "\n",
        "    ##### Positional encoding\n",
        "    pos_emb = relative_positional_encoding(\n",
        "        qlen, klen, d_model, clamp_len, attn_type, bi_data,\n",
        "        bsz=bsz, dtype=tf_float)\n",
        "    pos_emb = tf.layers.dropout(pos_emb, dropout, training=is_training)\n",
        "\n",
        "    ##### Attention layers\n",
        "    if mems is None:\n",
        "      mems = [None] * n_layer\n",
        "\n",
        "    for i in range(n_layer):\n",
        "      # cache new mems\n",
        "      new_mems.append(_cache_mem(output_h, mems[i], mem_len, reuse_len))\n",
        "\n",
        "      # segment bias\n",
        "      if seg_id is None:\n",
        "        r_s_bias_i = None\n",
        "        seg_embed_i = None\n",
        "      else:\n",
        "        r_s_bias_i = r_s_bias if not untie_r else r_s_bias[i]\n",
        "        seg_embed_i = seg_embed[i]\n",
        "\n",
        "      with tf.variable_scope('layer_{}'.format(i)):\n",
        "        if inp_q is not None:\n",
        "          output_h, output_g = two_stream_rel_attn(\n",
        "              h=output_h,\n",
        "              g=output_g,\n",
        "              r=pos_emb,\n",
        "              r_w_bias=r_w_bias if not untie_r else r_w_bias[i],\n",
        "              r_r_bias=r_r_bias if not untie_r else r_r_bias[i],\n",
        "              seg_mat=seg_mat,\n",
        "              r_s_bias=r_s_bias_i,\n",
        "              seg_embed=seg_embed_i,\n",
        "              attn_mask_h=non_tgt_mask,\n",
        "              attn_mask_g=attn_mask,\n",
        "              mems=mems[i],\n",
        "              target_mapping=target_mapping,\n",
        "              d_model=d_model,\n",
        "              n_head=n_head,\n",
        "              d_head=d_head,\n",
        "              dropout=dropout,\n",
        "              dropatt=dropatt,\n",
        "              is_training=is_training,\n",
        "              kernel_initializer=initializer)\n",
        "          reuse = True\n",
        "        else:\n",
        "          reuse = False\n",
        "\n",
        "          output_h = rel_multihead_attn(\n",
        "              h=output_h,\n",
        "              r=pos_emb,\n",
        "              r_w_bias=r_w_bias if not untie_r else r_w_bias[i],\n",
        "              r_r_bias=r_r_bias if not untie_r else r_r_bias[i],\n",
        "              seg_mat=seg_mat,\n",
        "              r_s_bias=r_s_bias_i,\n",
        "              seg_embed=seg_embed_i,\n",
        "              attn_mask=non_tgt_mask,\n",
        "              mems=mems[i],\n",
        "              d_model=d_model,\n",
        "              n_head=n_head,\n",
        "              d_head=d_head,\n",
        "              dropout=dropout,\n",
        "              dropatt=dropatt,\n",
        "              is_training=is_training,\n",
        "              kernel_initializer=initializer,\n",
        "              reuse=reuse)\n",
        "\n",
        "        if inp_q is not None:\n",
        "          output_g = positionwise_ffn(\n",
        "              inp=output_g,\n",
        "              d_model=d_model,\n",
        "              d_inner=d_inner,\n",
        "              dropout=dropout,\n",
        "              kernel_initializer=initializer,\n",
        "              activation_type=ff_activation,\n",
        "              is_training=is_training)\n",
        "\n",
        "        output_h = positionwise_ffn(\n",
        "            inp=output_h,\n",
        "            d_model=d_model,\n",
        "            d_inner=d_inner,\n",
        "            dropout=dropout,\n",
        "            kernel_initializer=initializer,\n",
        "            activation_type=ff_activation,\n",
        "            is_training=is_training,\n",
        "            reuse=reuse)\n",
        "\n",
        "    if inp_q is not None:\n",
        "      output = tf.layers.dropout(output_g, dropout, training=is_training)\n",
        "    else:\n",
        "      output = tf.layers.dropout(output_h, dropout, training=is_training)\n",
        "\n",
        "    return output, new_mems, lookup_table\n",
        "\n",
        "\n",
        "def lm_loss(hidden, target, n_token, d_model, initializer, lookup_table=None,\n",
        "            tie_weight=False, bi_data=True, use_tpu=False):\n",
        "  \"\"\"doc.\"\"\"\n",
        "\n",
        "  with tf.variable_scope('lm_loss'):\n",
        "    if tie_weight:\n",
        "      assert lookup_table is not None, \\\n",
        "          'lookup_table cannot be None for tie_weight'\n",
        "      softmax_w = lookup_table\n",
        "    else:\n",
        "      softmax_w = tf.get_variable('weight', [n_token, d_model],\n",
        "                                  dtype=hidden.dtype, initializer=initializer)\n",
        "\n",
        "    softmax_b = tf.get_variable('bias', [n_token], dtype=hidden.dtype,\n",
        "                                initializer=tf.zeros_initializer())\n",
        "\n",
        "    logits = tf.einsum('ibd,nd->ibn', hidden, softmax_w) + softmax_b\n",
        "\n",
        "    if use_tpu:\n",
        "      one_hot_target = tf.one_hot(target, n_token, dtype=logits.dtype)\n",
        "      loss = -tf.reduce_sum(tf.nn.log_softmax(logits) * one_hot_target, -1)\n",
        "    else:\n",
        "      loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target,\n",
        "                                                            logits=logits)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def summarize_sequence(summary_type, hidden, d_model, n_head, d_head, dropout,\n",
        "                       dropatt, input_mask, is_training, initializer,\n",
        "                       scope=None, reuse=None, use_proj=True):\n",
        "\n",
        "  \"\"\"\n",
        "      Different classification tasks may not may not share the same parameters\n",
        "      to summarize the sequence features.\n",
        "      If shared, one can keep the `scope` to the default value `None`.\n",
        "      Otherwise, one should specify a different `scope` for each task.\n",
        "  \"\"\"\n",
        "\n",
        "  with tf.variable_scope(scope, 'sequnece_summary', reuse=reuse):\n",
        "    if summary_type == 'last':\n",
        "      summary = hidden[-1]\n",
        "    elif summary_type == 'first':\n",
        "      summary = hidden[0]\n",
        "    elif summary_type == 'mean':\n",
        "      summary = tf.reduce_mean(hidden, axis=0)\n",
        "    elif summary_type == 'attn':\n",
        "      bsz = tf.shape(hidden)[1]\n",
        "\n",
        "      summary_bias = tf.get_variable('summary_bias', [d_model],\n",
        "                                     dtype=hidden.dtype,\n",
        "                                     initializer=initializer)\n",
        "      summary_bias = tf.tile(summary_bias[None, None], [1, bsz, 1])\n",
        "\n",
        "      if input_mask is not None:\n",
        "        input_mask = input_mask[None, :, :, None]\n",
        "\n",
        "      summary = multihead_attn(summary_bias, hidden, hidden, input_mask,\n",
        "                               d_model, n_head, d_head, dropout, dropatt,\n",
        "                               is_training, initializer, residual=False)\n",
        "      summary = summary[0]\n",
        "    else:\n",
        "      raise ValueError('Unsupported summary type {}'.format(summary_type))\n",
        "\n",
        "    # use another projection as in BERT\n",
        "    if use_proj:\n",
        "      summary = tf.layers.dense(\n",
        "          summary,\n",
        "          d_model,\n",
        "          activation=tf.tanh,\n",
        "          kernel_initializer=initializer,\n",
        "          name='summary')\n",
        "\n",
        "    # dropout\n",
        "    summary = tf.layers.dropout(\n",
        "        summary, dropout, training=is_training,\n",
        "        name='dropout')\n",
        "\n",
        "  return summary\n",
        "\n",
        "\n",
        "def classification_loss(hidden, labels, n_class, initializer, scope, reuse=None,\n",
        "                        return_logits=False):\n",
        "  \"\"\"\n",
        "      Different classification tasks should use different scope names to ensure\n",
        "      different dense layers (parameters) are used to produce the logits.\n",
        "      An exception will be in transfer learning, where one hopes to transfer\n",
        "      the classification weights.\n",
        "  \"\"\"\n",
        "\n",
        "  with tf.variable_scope(scope, reuse=reuse):\n",
        "    logits = tf.layers.dense(\n",
        "        hidden,\n",
        "        n_class,\n",
        "        kernel_initializer=initializer,\n",
        "        name='logit')\n",
        "\n",
        "    one_hot_target = tf.one_hot(labels, n_class, dtype=hidden.dtype)\n",
        "    loss = -tf.reduce_sum(tf.nn.log_softmax(logits) * one_hot_target, -1)\n",
        "\n",
        "    if return_logits:\n",
        "      return loss, logits\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def regression_loss(hidden, labels, initializer, scope, reuse=None,\n",
        "                    return_logits=False):\n",
        "  with tf.variable_scope(scope, reuse=reuse):\n",
        "    logits = tf.layers.dense(\n",
        "        hidden,\n",
        "        1,\n",
        "        kernel_initializer=initializer,\n",
        "        name='logit')\n",
        "\n",
        "    logits = tf.squeeze(logits, axis=-1)\n",
        "    loss = tf.square(logits - labels)\n",
        "\n",
        "    if return_logits:\n",
        "      return loss, logits\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j6vga5mjiMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "special_symbols = {\n",
        "    \"<unk>\"  : 0,\n",
        "    \"<s>\"    : 1,\n",
        "    \"</s>\"   : 2,\n",
        "    \"<cls>\"  : 3,\n",
        "    \"<sep>\"  : 4,\n",
        "    \"<pad>\"  : 5,\n",
        "    \"<mask>\" : 6,\n",
        "    \"<eod>\"  : 7,\n",
        "    \"<eop>\"  : 8,\n",
        "}\n",
        "\n",
        "VOCAB_SIZE = 32000\n",
        "UNK_ID = special_symbols[\"<unk>\"]\n",
        "CLS_ID = special_symbols[\"<cls>\"]\n",
        "SEP_ID = special_symbols[\"<sep>\"]\n",
        "MASK_ID = special_symbols[\"<mask>\"]\n",
        "EOD_ID = special_symbols[\"<eod>\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1qn7MNhPxja",
        "colab_type": "code",
        "outputId": "066486c7-a576-4027-c213-b1750dc300d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"Official evaluation script for SQuAD version 2.0.\n",
        "In addition to basic functionality, we also compute additional statistics and\n",
        "plot precision-recall curves if an additional na_prob.json file is provided.\n",
        "This file is expected to map question ID's to the model's predicted probability\n",
        "that a question is unanswerable.\n",
        "\"\"\"\n",
        "import argparse\n",
        "import collections\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "\n",
        "OPTS = None\n",
        "\n",
        "def parse_args():\n",
        "  parser = argparse.ArgumentParser('Official evaluation script for SQuAD version 2.0.')\n",
        "  parser.add_argument('data_file', metavar='data.json', help='Input data JSON file.')\n",
        "  parser.add_argument('pred_file', metavar='pred.json', help='Model predictions.')\n",
        "  parser.add_argument('--out-file', '-o', metavar='eval.json',\n",
        "                      help='Write accuracy metrics to file (default is stdout).')\n",
        "  parser.add_argument('--na-prob-file', '-n', metavar='na_prob.json',\n",
        "                      help='Model estimates of probability of no answer.')\n",
        "  parser.add_argument('--na-prob-thresh', '-t', type=float, default=1.0,\n",
        "                      help='Predict \"\" if no-answer probability exceeds this (default = 1.0).')\n",
        "  parser.add_argument('--out-image-dir', '-p', metavar='out_images', default=None,\n",
        "                      help='Save precision-recall curves to directory.')\n",
        "  parser.add_argument('--verbose', '-v', action='store_true')\n",
        "  if len(sys.argv) == 1:\n",
        "    parser.print_help()\n",
        "    sys.exit(1)\n",
        "  return parser.parse_args()\n",
        "\n",
        "def make_qid_to_has_ans(dataset):\n",
        "  qid_to_has_ans = {}\n",
        "  for article in dataset:\n",
        "    for p in article['paragraphs']:\n",
        "      for qa in p['qas']:\n",
        "        qid_to_has_ans[qa['id']] = bool(qa['answers'])\n",
        "  return qid_to_has_ans\n",
        "\n",
        "def normalize_answer(s):\n",
        "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "  def remove_articles(text):\n",
        "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "    return re.sub(regex, ' ', text)\n",
        "  def white_space_fix(text):\n",
        "    return ' '.join(text.split())\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def get_tokens(s):\n",
        "  if not s: return []\n",
        "  return normalize_answer(s).split()\n",
        "\n",
        "def compute_exact(a_gold, a_pred):\n",
        "  return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
        "\n",
        "def compute_f1(a_gold, a_pred):\n",
        "  gold_toks = get_tokens(a_gold)\n",
        "  pred_toks = get_tokens(a_pred)\n",
        "  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "  num_same = sum(common.values())\n",
        "  if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "    return int(gold_toks == pred_toks)\n",
        "  if num_same == 0:\n",
        "    return 0\n",
        "  precision = 1.0 * num_same / len(pred_toks)\n",
        "  recall = 1.0 * num_same / len(gold_toks)\n",
        "  f1 = (2 * precision * recall) / (precision + recall)\n",
        "  return f1\n",
        "\n",
        "def get_raw_scores(dataset, preds):\n",
        "  exact_scores = {}\n",
        "  f1_scores = {}\n",
        "  for article in dataset:\n",
        "    for p in article['paragraphs']:\n",
        "      for qa in p['qas']:\n",
        "        qid = qa['id']\n",
        "        gold_answers = [a['text'] for a in qa['answers']\n",
        "                        if normalize_answer(a['text'])]\n",
        "        if not gold_answers:\n",
        "          # For unanswerable questions, only correct answer is empty string\n",
        "          gold_answers = ['']\n",
        "        if qid not in preds:\n",
        "          print('Missing prediction for %s' % qid)\n",
        "          continue\n",
        "        a_pred = preds[qid]\n",
        "        # Take max over all gold answers\n",
        "        exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n",
        "        f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n",
        "  return exact_scores, f1_scores\n",
        "\n",
        "def apply_no_ans_threshold(scores, na_probs, qid_to_has_ans, na_prob_thresh):\n",
        "  new_scores = {}\n",
        "  for qid, s in scores.items():\n",
        "    pred_na = na_probs[qid] > na_prob_thresh\n",
        "    if pred_na:\n",
        "      new_scores[qid] = float(not qid_to_has_ans[qid])\n",
        "    else:\n",
        "      new_scores[qid] = s\n",
        "  return new_scores\n",
        "\n",
        "def make_eval_dict(exact_scores, f1_scores, qid_list=None):\n",
        "  if not qid_list:\n",
        "    total = len(exact_scores)\n",
        "    return collections.OrderedDict([\n",
        "        ('exact', 100.0 * sum(exact_scores.values()) / total),\n",
        "        ('f1', 100.0 * sum(f1_scores.values()) / total),\n",
        "        ('total', total),\n",
        "    ])\n",
        "  else:\n",
        "    total = len(qid_list)\n",
        "    return collections.OrderedDict([\n",
        "        ('exact', 100.0 * sum(exact_scores[k] for k in qid_list) / total),\n",
        "        ('f1', 100.0 * sum(f1_scores[k] for k in qid_list) / total),\n",
        "        ('total', total),\n",
        "    ])\n",
        "\n",
        "def merge_eval(main_eval, new_eval, prefix):\n",
        "  for k in new_eval:\n",
        "    main_eval['%s_%s' % (prefix, k)] = new_eval[k]\n",
        "\n",
        "def plot_pr_curve(precisions, recalls, out_image, title):\n",
        "  plt.step(recalls, precisions, color='b', alpha=0.2, where='post')\n",
        "  plt.fill_between(recalls, precisions, step='post', alpha=0.2, color='b')\n",
        "  plt.xlabel('Recall')\n",
        "  plt.ylabel('Precision')\n",
        "  plt.xlim([0.0, 1.05])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.title(title)\n",
        "  plt.savefig(out_image)\n",
        "  plt.clf()\n",
        "\n",
        "def make_precision_recall_eval(scores, na_probs, num_true_pos, qid_to_has_ans,\n",
        "                               out_image=None, title=None):\n",
        "  qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n",
        "  true_pos = 0.0\n",
        "  cur_p = 1.0\n",
        "  cur_r = 0.0\n",
        "  precisions = [1.0]\n",
        "  recalls = [0.0]\n",
        "  avg_prec = 0.0\n",
        "  for i, qid in enumerate(qid_list):\n",
        "    if qid_to_has_ans[qid]:\n",
        "      true_pos += scores[qid]\n",
        "    cur_p = true_pos / float(i+1)\n",
        "    cur_r = true_pos / float(num_true_pos)\n",
        "    if i == len(qid_list) - 1 or na_probs[qid] != na_probs[qid_list[i+1]]:\n",
        "      # i.e., if we can put a threshold after this point\n",
        "      avg_prec += cur_p * (cur_r - recalls[-1])\n",
        "      precisions.append(cur_p)\n",
        "      recalls.append(cur_r)\n",
        "  if out_image:\n",
        "    plot_pr_curve(precisions, recalls, out_image, title)\n",
        "  return {'ap': 100.0 * avg_prec}\n",
        "\n",
        "def run_precision_recall_analysis(main_eval, exact_raw, f1_raw, na_probs, \n",
        "                                  qid_to_has_ans, out_image_dir):\n",
        "  if out_image_dir and not os.path.exists(out_image_dir):\n",
        "    os.makedirs(out_image_dir)\n",
        "  num_true_pos = sum(1 for v in qid_to_has_ans.values() if v)\n",
        "  if num_true_pos == 0:\n",
        "    return\n",
        "  pr_exact = make_precision_recall_eval(\n",
        "      exact_raw, na_probs, num_true_pos, qid_to_has_ans,\n",
        "      out_image=os.path.join(out_image_dir, 'pr_exact.png'),\n",
        "      title='Precision-Recall curve for Exact Match score')\n",
        "  pr_f1 = make_precision_recall_eval(\n",
        "      f1_raw, na_probs, num_true_pos, qid_to_has_ans,\n",
        "      out_image=os.path.join(out_image_dir, 'pr_f1.png'),\n",
        "      title='Precision-Recall curve for F1 score')\n",
        "  oracle_scores = {k: float(v) for k, v in qid_to_has_ans.items()}\n",
        "  pr_oracle = make_precision_recall_eval(\n",
        "      oracle_scores, na_probs, num_true_pos, qid_to_has_ans,\n",
        "      out_image=os.path.join(out_image_dir, 'pr_oracle.png'),\n",
        "      title='Oracle Precision-Recall curve (binary task of HasAns vs. NoAns)')\n",
        "  merge_eval(main_eval, pr_exact, 'pr_exact')\n",
        "  merge_eval(main_eval, pr_f1, 'pr_f1')\n",
        "  merge_eval(main_eval, pr_oracle, 'pr_oracle')\n",
        "\n",
        "def histogram_na_prob(na_probs, qid_list, image_dir, name):\n",
        "  if not qid_list:\n",
        "    return\n",
        "  x = [na_probs[k] for k in qid_list]\n",
        "  weights = np.ones_like(x) / float(len(x))\n",
        "  plt.hist(x, weights=weights, bins=20, range=(0.0, 1.0))\n",
        "  plt.xlabel('Model probability of no-answer')\n",
        "  plt.ylabel('Proportion of dataset')\n",
        "  plt.title('Histogram of no-answer probability: %s' % name)\n",
        "  plt.savefig(os.path.join(image_dir, 'na_prob_hist_%s.png' % name))\n",
        "  plt.clf()\n",
        "\n",
        "def find_best_thresh(preds, scores, na_probs, qid_to_has_ans):\n",
        "  num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n",
        "  cur_score = num_no_ans\n",
        "  best_score = cur_score\n",
        "  best_thresh = 0.0\n",
        "  qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n",
        "  for i, qid in enumerate(qid_list):\n",
        "    if qid not in scores: continue\n",
        "    if qid_to_has_ans[qid]:\n",
        "      diff = scores[qid]\n",
        "    else:\n",
        "      if preds[qid]:\n",
        "        diff = -1\n",
        "      else:\n",
        "        diff = 0\n",
        "    cur_score += diff\n",
        "    if cur_score > best_score:\n",
        "      best_score = cur_score\n",
        "      best_thresh = na_probs[qid]\n",
        "  return 100.0 * best_score / len(scores), best_thresh\n",
        "\n",
        "def find_best_thresh_v2(preds, scores, na_probs, qid_to_has_ans):\n",
        "  num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n",
        "  cur_score = num_no_ans\n",
        "  best_score = cur_score\n",
        "  best_thresh = 0.0\n",
        "  qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n",
        "  for i, qid in enumerate(qid_list):\n",
        "    if qid not in scores: continue\n",
        "    if qid_to_has_ans[qid]:\n",
        "      diff = scores[qid]\n",
        "    else:\n",
        "      if preds[qid]:\n",
        "        diff = -1\n",
        "      else:\n",
        "        diff = 0\n",
        "    cur_score += diff\n",
        "    if cur_score > best_score:\n",
        "      best_score = cur_score\n",
        "      best_thresh = na_probs[qid]\n",
        "\n",
        "  has_ans_score, has_ans_cnt = 0, 0\n",
        "  for qid in qid_list:\n",
        "    if not qid_to_has_ans[qid]: continue\n",
        "    has_ans_cnt += 1\n",
        "\n",
        "    if qid not in scores: continue\n",
        "    has_ans_score += scores[qid]\n",
        "\n",
        "  return 100.0 * best_score / len(scores), best_thresh, 1.0 * has_ans_score / has_ans_cnt\n",
        "\n",
        "def find_all_best_thresh(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n",
        "  best_exact, exact_thresh = find_best_thresh(preds, exact_raw, na_probs, qid_to_has_ans)\n",
        "  best_f1, f1_thresh = find_best_thresh(preds, f1_raw, na_probs, qid_to_has_ans)\n",
        "  main_eval['best_exact'] = best_exact\n",
        "  main_eval['best_exact_thresh'] = exact_thresh\n",
        "  main_eval['best_f1'] = best_f1\n",
        "  main_eval['best_f1_thresh'] = f1_thresh\n",
        "\n",
        "def find_all_best_thresh_v2(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n",
        "  best_exact, exact_thresh, has_ans_exact = find_best_thresh_v2(preds, exact_raw, na_probs, qid_to_has_ans)\n",
        "  best_f1, f1_thresh, has_ans_f1 = find_best_thresh_v2(preds, f1_raw, na_probs, qid_to_has_ans)\n",
        "  main_eval['best_exact'] = best_exact\n",
        "  main_eval['best_exact_thresh'] = exact_thresh\n",
        "  main_eval['best_f1'] = best_f1\n",
        "  main_eval['best_f1_thresh'] = f1_thresh\n",
        "  main_eval['has_ans_exact'] = has_ans_exact\n",
        "  main_eval['has_ans_f1'] = has_ans_f1\n",
        "\n",
        "def main():\n",
        "  with open(OPTS.data_file) as f:\n",
        "    dataset_json = json.load(f)\n",
        "    dataset = dataset_json['data']\n",
        "  with open(OPTS.pred_file) as f:\n",
        "    preds = json.load(f)\n",
        "\n",
        "  new_orig_data = []\n",
        "  for article in dataset:\n",
        "    for p in article['paragraphs']:\n",
        "      for qa in p['qas']:\n",
        "        if qa['id'] in preds:\n",
        "          new_para = {'qas': [qa]}\n",
        "          new_article = {'paragraphs': [new_para]}\n",
        "          new_orig_data.append(new_article)\n",
        "  dataset = new_orig_data\n",
        "\n",
        "  if OPTS.na_prob_file:\n",
        "    with open(OPTS.na_prob_file) as f:\n",
        "      na_probs = json.load(f)\n",
        "  else:\n",
        "    na_probs = {k: 0.0 for k in preds}\n",
        "  qid_to_has_ans = make_qid_to_has_ans(dataset)  # maps qid to True/False\n",
        "  has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n",
        "  no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n",
        "  exact_raw, f1_raw = get_raw_scores(dataset, preds)\n",
        "  exact_thresh = apply_no_ans_threshold(exact_raw, na_probs, qid_to_has_ans,\n",
        "                                        OPTS.na_prob_thresh)\n",
        "  f1_thresh = apply_no_ans_threshold(f1_raw, na_probs, qid_to_has_ans,\n",
        "                                     OPTS.na_prob_thresh)\n",
        "  out_eval = make_eval_dict(exact_thresh, f1_thresh)\n",
        "  if has_ans_qids:\n",
        "    has_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=has_ans_qids)\n",
        "    merge_eval(out_eval, has_ans_eval, 'HasAns')\n",
        "  if no_ans_qids:\n",
        "    no_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=no_ans_qids)\n",
        "    merge_eval(out_eval, no_ans_eval, 'NoAns')\n",
        "  if OPTS.na_prob_file:\n",
        "    find_all_best_thresh(out_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans)\n",
        "  if OPTS.na_prob_file and OPTS.out_image_dir:\n",
        "    run_precision_recall_analysis(out_eval, exact_raw, f1_raw, na_probs, \n",
        "                                  qid_to_has_ans, OPTS.out_image_dir)\n",
        "    histogram_na_prob(na_probs, has_ans_qids, OPTS.out_image_dir, 'hasAns')\n",
        "    histogram_na_prob(na_probs, no_ans_qids, OPTS.out_image_dir, 'noAns')\n",
        "  if OPTS.out_file:\n",
        "    with open(OPTS.out_file, 'w') as f:\n",
        "      json.dump(out_eval, f)\n",
        "  else:\n",
        "    print(json.dumps(out_eval, indent=2))\n",
        "'''\n",
        "if __name__ == '__main__':\n",
        "  OPTS = parse_args()\n",
        "  if OPTS.out_image_dir:\n",
        "    import matplotlib\n",
        "    matplotlib.use('Agg')\n",
        "    import matplotlib.pyplot as plt \n",
        "  main()\n",
        "  \n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nif __name__ == '__main__':\\n  OPTS = parse_args()\\n  if OPTS.out_image_dir:\\n    import matplotlib\\n    matplotlib.use('Agg')\\n    import matplotlib.pyplot as plt \\n  main()\\n  \\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEdj8O7ELj_y",
        "colab_type": "code",
        "outputId": "4f8d9e05-d45c-4f61-9fa0-80c545f808a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import six\n",
        "from os.path import join\n",
        "from six.moves import zip\n",
        "\n",
        "from absl import flags\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def configure_tpu(FLAGS):\n",
        "  if FLAGS.use_tpu:\n",
        "    tpu_cluster = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
        "        FLAGS.tpu, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n",
        "    master = tpu_cluster.get_master()\n",
        "  else:\n",
        "    tpu_cluster = None\n",
        "    master = FLAGS.master\n",
        "\n",
        "  session_config = tf.ConfigProto(allow_soft_placement=True)\n",
        "  # Uncomment the following line if you hope to monitor GPU RAM growth\n",
        "  # session_config.gpu_options.allow_growth = True\n",
        "\n",
        "  if FLAGS.use_tpu:\n",
        "    strategy = None\n",
        "    tf.logging.info('Use TPU without distribute strategy.')\n",
        "  elif FLAGS.num_core_per_host == 1:\n",
        "    strategy = None\n",
        "    tf.logging.info('Single device mode.')\n",
        "  else:\n",
        "    strategy = tf.contrib.distribute.MirroredStrategy(\n",
        "        num_gpus=FLAGS.num_core_per_host)\n",
        "    tf.logging.info('Use MirroredStrategy with %d devices.',\n",
        "                    strategy.num_replicas_in_sync)\n",
        "\n",
        "  per_host_input = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
        "  run_config = tf.contrib.tpu.RunConfig(\n",
        "      master=master,\n",
        "      model_dir=FLAGS.model_dir,\n",
        "      session_config=session_config,\n",
        "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "          iterations_per_loop=FLAGS.iterations,\n",
        "          num_shards=FLAGS.num_hosts * FLAGS.num_core_per_host,\n",
        "          per_host_input_for_training=per_host_input),\n",
        "      keep_checkpoint_max=FLAGS.max_save,\n",
        "      save_checkpoints_secs=None,\n",
        "      save_checkpoints_steps=FLAGS.save_steps,\n",
        "      train_distribute=strategy\n",
        "  )\n",
        "  return run_config\n",
        "\n",
        "\n",
        "def init_from_checkpoint(FLAGS, global_vars=False):\n",
        "  tvars = tf.global_variables() if global_vars else tf.trainable_variables()\n",
        "  initialized_variable_names = {}\n",
        "  scaffold_fn = None\n",
        "  if FLAGS.init_checkpoint is not None:\n",
        "    if FLAGS.init_checkpoint.endswith(\"latest\"):\n",
        "      ckpt_dir = os.path.dirname(FLAGS.init_checkpoint)\n",
        "      init_checkpoint = tf.train.latest_checkpoint(ckpt_dir)\n",
        "    else:\n",
        "      init_checkpoint = FLAGS.init_checkpoint\n",
        "\n",
        "    tf.logging.info(\"Initialize from the ckpt {}\".format(init_checkpoint))\n",
        "\n",
        "    (assignment_map, initialized_variable_names\n",
        "    ) = get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "    if FLAGS.use_tpu:\n",
        "      def tpu_scaffold():\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "        return tf.train.Scaffold()\n",
        "\n",
        "      scaffold_fn = tpu_scaffold\n",
        "    else:\n",
        "      tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    # Log customized initialization\n",
        "    tf.logging.info(\"**** Global Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "  return scaffold_fn\n",
        "\n",
        "\n",
        "def get_train_op(FLAGS, total_loss, grads_and_vars=None):\n",
        "  global_step = tf.train.get_or_create_global_step()\n",
        "\n",
        "  # increase the learning rate linearly\n",
        "  if FLAGS.warmup_steps > 0:\n",
        "    warmup_lr = (tf.cast(global_step, tf.float32)\n",
        "                 / tf.cast(FLAGS.warmup_steps, tf.float32)\n",
        "                 * FLAGS.learning_rate)\n",
        "  else:\n",
        "    warmup_lr = 0.0\n",
        "\n",
        "  # decay the learning rate\n",
        "  if FLAGS.decay_method == \"poly\":\n",
        "    decay_lr = tf.train.polynomial_decay(\n",
        "        FLAGS.learning_rate,\n",
        "        global_step=global_step - FLAGS.warmup_steps,\n",
        "        decay_steps=FLAGS.train_steps - FLAGS.warmup_steps,\n",
        "        end_learning_rate=FLAGS.learning_rate * FLAGS.min_lr_ratio)\n",
        "  elif FLAGS.decay_method == \"cos\":\n",
        "    decay_lr = tf.train.cosine_decay(\n",
        "        FLAGS.learning_rate,\n",
        "        global_step=global_step - FLAGS.warmup_steps,\n",
        "        decay_steps=FLAGS.train_steps - FLAGS.warmup_steps,\n",
        "        alpha=FLAGS.min_lr_ratio)\n",
        "  else:\n",
        "    raise ValueError(FLAGS.decay_method)\n",
        "\n",
        "  learning_rate = tf.where(global_step < FLAGS.warmup_steps,\n",
        "                           warmup_lr, decay_lr)\n",
        "\n",
        "  if (FLAGS.weight_decay > 0 and not FLAGS.use_tpu and\n",
        "      FLAGS.num_core_per_host > 1):\n",
        "    raise ValueError(\"Do not support `weight_decay > 0` with multi-gpu \"\n",
        "                     \"training so far.\")\n",
        "\n",
        "  if FLAGS.weight_decay == 0:\n",
        "    optimizer = tf.train.AdamOptimizer(\n",
        "        learning_rate=learning_rate,\n",
        "        epsilon=FLAGS.adam_epsilon)\n",
        "  else:\n",
        "    optimizer = AdamWeightDecayOptimizer(\n",
        "        learning_rate=learning_rate,\n",
        "        epsilon=FLAGS.adam_epsilon,\n",
        "        exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"],\n",
        "        weight_decay_rate=FLAGS.weight_decay)\n",
        "\n",
        "  if FLAGS.use_tpu:\n",
        "    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
        "\n",
        "  if grads_and_vars is None:\n",
        "    grads_and_vars = optimizer.compute_gradients(total_loss)\n",
        "  gradients, variables = zip(*grads_and_vars)\n",
        "  clipped, gnorm = tf.clip_by_global_norm(gradients, FLAGS.clip)\n",
        "\n",
        "  if getattr(FLAGS, \"lr_layer_decay_rate\", 1.0) != 1.0:\n",
        "    n_layer = 0\n",
        "    for i in range(len(clipped)):\n",
        "      m = re.search(r\"model/transformer/layer_(\\d+?)/\", variables[i].name)\n",
        "      if not m: continue\n",
        "      n_layer = max(n_layer, int(m.group(1)) + 1)\n",
        "\n",
        "    for i in range(len(clipped)):\n",
        "      for l in range(n_layer):\n",
        "        if \"model/transformer/layer_{}/\".format(l) in variables[i].name:\n",
        "          abs_rate = FLAGS.lr_layer_decay_rate ** (n_layer - 1 - l)\n",
        "          clipped[i] *= abs_rate\n",
        "          tf.logging.info(\"Apply mult {:.4f} to layer-{} grad of {}\".format(\n",
        "              abs_rate, l, variables[i].name))\n",
        "          break\n",
        "\n",
        "  train_op = optimizer.apply_gradients(\n",
        "      zip(clipped, variables), global_step=global_step)\n",
        "\n",
        "  # Manually increment `global_step` for AdamWeightDecayOptimizer\n",
        "  if FLAGS.weight_decay > 0:\n",
        "    new_global_step = global_step + 1\n",
        "    train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n",
        "\n",
        "  return train_op, learning_rate, gnorm\n",
        "\n",
        "\n",
        "def clean_ckpt(_):\n",
        "  input_ckpt = FLAGS.clean_input_ckpt\n",
        "  output_model_dir = FLAGS.clean_output_model_dir\n",
        "\n",
        "  tf.reset_default_graph()\n",
        "\n",
        "  var_list = tf.contrib.framework.list_variables(input_ckpt)\n",
        "  var_values, var_dtypes = {}, {}\n",
        "  for (name, shape) in var_list:\n",
        "    if not name.startswith(\"global_step\") and \"adam\" not in name.lower():\n",
        "      var_values[name] = None\n",
        "      tf.logging.info(\"Include {}\".format(name))\n",
        "    else:\n",
        "      tf.logging.info(\"Exclude {}\".format(name))\n",
        "\n",
        "  tf.logging.info(\"Loading from {}\".format(input_ckpt))\n",
        "  reader = tf.contrib.framework.load_checkpoint(input_ckpt)\n",
        "  for name in var_values:\n",
        "    tensor = reader.get_tensor(name)\n",
        "    var_dtypes[name] = tensor.dtype\n",
        "    var_values[name] = tensor\n",
        "\n",
        "  with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
        "    tf_vars = [\n",
        "        tf.get_variable(v, shape=var_values[v].shape, dtype=var_dtypes[v])\n",
        "        for v in var_values\n",
        "    ]\n",
        "  placeholders = [tf.placeholder(v.dtype, shape=v.shape) for v in tf_vars]\n",
        "  assign_ops = [tf.assign(v, p) for (v, p) in zip(tf_vars, placeholders)]\n",
        "  global_step = tf.Variable(\n",
        "      0, name=\"global_step\", trainable=False, dtype=tf.int64)\n",
        "  saver = tf.train.Saver(tf.all_variables())\n",
        "\n",
        "  if not tf.gfile.Exists(output_model_dir):\n",
        "    tf.gfile.MakeDirs(output_model_dir)\n",
        "\n",
        "  # Build a model consisting only of variables, set them to the average values.\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.initialize_all_variables())\n",
        "    for p, assign_op, (name, value) in zip(placeholders, assign_ops,\n",
        "                                           six.iteritems(var_values)):\n",
        "      sess.run(assign_op, {p: value})\n",
        "\n",
        "    # Use the built saver to save the averaged checkpoint.\n",
        "    saver.save(sess, join(output_model_dir, \"model.ckpt\"),\n",
        "               global_step=global_step)\n",
        "\n",
        "\n",
        "def avg_checkpoints(model_dir, output_model_dir, last_k):\n",
        "  tf.reset_default_graph()\n",
        "\n",
        "  checkpoint_state = tf.train.get_checkpoint_state(model_dir)\n",
        "  checkpoints = checkpoint_state.all_model_checkpoint_paths[- last_k:]\n",
        "  var_list = tf.contrib.framework.list_variables(checkpoints[0])\n",
        "  var_values, var_dtypes = {}, {}\n",
        "  for (name, shape) in var_list:\n",
        "    if not name.startswith(\"global_step\"):\n",
        "      var_values[name] = np.zeros(shape)\n",
        "  for checkpoint in checkpoints:\n",
        "    reader = tf.contrib.framework.load_checkpoint(checkpoint)\n",
        "    for name in var_values:\n",
        "      tensor = reader.get_tensor(name)\n",
        "      var_dtypes[name] = tensor.dtype\n",
        "      var_values[name] += tensor\n",
        "    tf.logging.info(\"Read from checkpoint %s\", checkpoint)\n",
        "  for name in var_values:  # Average.\n",
        "    var_values[name] /= len(checkpoints)\n",
        "\n",
        "  with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
        "    tf_vars = [\n",
        "        tf.get_variable(v, shape=var_values[v].shape, dtype=var_dtypes[v])\n",
        "        for v in var_values\n",
        "    ]\n",
        "  placeholders = [tf.placeholder(v.dtype, shape=v.shape) for v in tf_vars]\n",
        "  assign_ops = [tf.assign(v, p) for (v, p) in zip(tf_vars, placeholders)]\n",
        "  global_step = tf.Variable(\n",
        "      0, name=\"global_step\", trainable=False, dtype=tf.int64)\n",
        "  saver = tf.train.Saver(tf.all_variables())\n",
        "\n",
        "  # Build a model consisting only of variables, set them to the average values.\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.initialize_all_variables())\n",
        "    for p, assign_op, (name, value) in zip(placeholders, assign_ops,\n",
        "                                           six.iteritems(var_values)):\n",
        "      sess.run(assign_op, {p: value})\n",
        "    # Use the built saver to save the averaged checkpoint.\n",
        "    saver.save(sess, join(output_model_dir, \"model.ckpt\"),\n",
        "        global_step=global_step)\n",
        "\n",
        "\n",
        "def get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n",
        "  \"\"\"Compute the union of the current variables and checkpoint variables.\"\"\"\n",
        "  assignment_map = {}\n",
        "  initialized_variable_names = {}\n",
        "\n",
        "  name_to_variable = collections.OrderedDict()\n",
        "  for var in tvars:\n",
        "    name = var.name\n",
        "    m = re.match(\"^(.*):\\\\d+$\", name)\n",
        "    if m is not None:\n",
        "      name = m.group(1)\n",
        "    name_to_variable[name] = var\n",
        "\n",
        "  init_vars = tf.train.list_variables(init_checkpoint)\n",
        "\n",
        "  assignment_map = collections.OrderedDict()\n",
        "  for x in init_vars:\n",
        "    (name, var) = (x[0], x[1])\n",
        "    # tf.logging.info('original name: %s', name)\n",
        "    if name not in name_to_variable:\n",
        "      continue\n",
        "    # assignment_map[name] = name\n",
        "    assignment_map[name] = name_to_variable[name]\n",
        "    initialized_variable_names[name] = 1\n",
        "    initialized_variable_names[name + \":0\"] = 1\n",
        "\n",
        "  return (assignment_map, initialized_variable_names)\n",
        "\n",
        "\n",
        "class AdamWeightDecayOptimizer(tf.train.Optimizer):\n",
        "  \"\"\"A basic Adam optimizer that includes \"correct\" L2 weight decay.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               learning_rate,\n",
        "               weight_decay_rate=0.0,\n",
        "               beta_1=0.9,\n",
        "               beta_2=0.999,\n",
        "               epsilon=1e-6,\n",
        "               exclude_from_weight_decay=None,\n",
        "               include_in_weight_decay=[\"r_s_bias\", \"r_r_bias\", \"r_w_bias\"],\n",
        "               name=\"AdamWeightDecayOptimizer\"):\n",
        "    \"\"\"Constructs a AdamWeightDecayOptimizer.\"\"\"\n",
        "    super(AdamWeightDecayOptimizer, self).__init__(False, name)\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.weight_decay_rate = weight_decay_rate\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "    self.epsilon = epsilon\n",
        "    self.exclude_from_weight_decay = exclude_from_weight_decay\n",
        "    self.include_in_weight_decay = include_in_weight_decay\n",
        "\n",
        "  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    assignments = []\n",
        "    for (grad, param) in grads_and_vars:\n",
        "      if grad is None or param is None:\n",
        "        continue\n",
        "\n",
        "      param_name = self._get_variable_name(param.name)\n",
        "\n",
        "      m = tf.get_variable(\n",
        "          name=param_name + \"/adam_m\",\n",
        "          shape=param.shape.as_list(),\n",
        "          dtype=tf.float32,\n",
        "          trainable=False,\n",
        "          initializer=tf.zeros_initializer())\n",
        "      v = tf.get_variable(\n",
        "          name=param_name + \"/adam_v\",\n",
        "          shape=param.shape.as_list(),\n",
        "          dtype=tf.float32,\n",
        "          trainable=False,\n",
        "          initializer=tf.zeros_initializer())\n",
        "\n",
        "      # Standard Adam update.\n",
        "      next_m = (\n",
        "          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))\n",
        "      next_v = (\n",
        "          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n",
        "                                                    tf.square(grad)))\n",
        "\n",
        "      update = next_m / (tf.sqrt(next_v) + self.epsilon)\n",
        "\n",
        "      # Just adding the square of the weights to the loss function is *not*\n",
        "      # the correct way of using L2 regularization/weight decay with Adam,\n",
        "      # since that will interact with the m and v parameters in strange ways.\n",
        "      #\n",
        "      # Instead we want ot decay the weights in a manner that doesn't interact\n",
        "      # with the m/v parameters. This is equivalent to adding the square\n",
        "      # of the weights to the loss with plain (non-momentum) SGD.\n",
        "      if self._do_use_weight_decay(param_name):\n",
        "        update += self.weight_decay_rate * param\n",
        "\n",
        "      update_with_lr = self.learning_rate * update\n",
        "\n",
        "      next_param = param - update_with_lr\n",
        "\n",
        "      assignments.extend(\n",
        "          [param.assign(next_param),\n",
        "           m.assign(next_m),\n",
        "           v.assign(next_v)])\n",
        "\n",
        "    return tf.group(*assignments, name=name)\n",
        "\n",
        "  def _do_use_weight_decay(self, param_name):\n",
        "    \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
        "    if not self.weight_decay_rate:\n",
        "      return False\n",
        "    for r in self.include_in_weight_decay:\n",
        "      if re.search(r, param_name) is not None:\n",
        "        return True\n",
        "\n",
        "    if self.exclude_from_weight_decay:\n",
        "      for r in self.exclude_from_weight_decay:\n",
        "        if re.search(r, param_name) is not None:\n",
        "          tf.logging.info('Adam WD excludes {}'.format(param_name))\n",
        "          return False\n",
        "    return True\n",
        "\n",
        "  def _get_variable_name(self, param_name):\n",
        "    \"\"\"Get the variable name from the tensor name.\"\"\"\n",
        "    m = re.match(\"^(.*):\\\\d+$\", param_name)\n",
        "    if m is not None:\n",
        "      param_name = m.group(1)\n",
        "    return param_name\n",
        "\n",
        "'''\n",
        "if __name__ == \"__main__\":\n",
        "  flags.DEFINE_string(\"clean_input_ckpt\", \"\", \"input ckpt for cleaning\")\n",
        "  flags.DEFINE_string(\"clean_output_model_dir\", \"\", \"output dir for cleaned ckpt\")\n",
        "\n",
        "  FLAGS = flags.FLAGS\n",
        "\n",
        "  tf.app.run(clean_ckpt)\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nif __name__ == \"__main__\":\\n  flags.DEFINE_string(\"clean_input_ckpt\", \"\", \"input ckpt for cleaning\")\\n  flags.DEFINE_string(\"clean_output_model_dir\", \"\", \"output dir for cleaned ckpt\")\\n\\n  FLAGS = flags.FLAGS\\n\\n  tf.app.run(clean_ckpt)\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xpf8kmiZhDrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import json\n",
        "import os\n",
        "import tensorflow as tf\n",
        "#import modeling\n",
        "\n",
        "\n",
        "def _get_initializer(FLAGS):\n",
        "  \"\"\"Get variable intializer.\"\"\"\n",
        "  if FLAGS.init == \"uniform\":\n",
        "    initializer = tf.initializers.random_uniform(\n",
        "        minval=-FLAGS.init_range,\n",
        "        maxval=FLAGS.init_range,\n",
        "        seed=None)\n",
        "  elif FLAGS.init == \"normal\":\n",
        "    initializer = tf.initializers.random_normal(\n",
        "        stddev=FLAGS.init_std,\n",
        "        seed=None)\n",
        "  else:\n",
        "    raise ValueError(\"Initializer {} not supported\".format(FLAGS.init))\n",
        "  return initializer\n",
        "\n",
        "\n",
        "class XLNetConfig(object):\n",
        "  \"\"\"XLNetConfig contains hyperparameters that are specific to a model checkpoint;\n",
        "  i.e., these hyperparameters should be the same between\n",
        "  pretraining and finetuning.\n",
        "  The following hyperparameters are defined:\n",
        "    n_layer: int, the number of layers.\n",
        "    d_model: int, the hidden size.\n",
        "    n_head: int, the number of attention heads.\n",
        "    d_head: int, the dimension size of each attention head.\n",
        "    d_inner: int, the hidden size in feed-forward layers.\n",
        "    ff_activation: str, \"relu\" or \"gelu\".\n",
        "    untie_r: bool, whether to untie the biases in attention.\n",
        "    n_token: int, the vocab size.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, FLAGS=None, json_path=None):\n",
        "    \"\"\"Constructing an XLNetConfig.\n",
        "    One of FLAGS or json_path should be provided.\"\"\"\n",
        "\n",
        "    assert FLAGS is not None or json_path is not None\n",
        "\n",
        "    self.keys = [\"n_layer\", \"d_model\", \"n_head\", \"d_head\", \"d_inner\",\n",
        "                 \"ff_activation\", \"untie_r\", \"n_token\"]\n",
        "\n",
        "    if FLAGS is not None:\n",
        "      self.init_from_flags(FLAGS)\n",
        "\n",
        "    if json_path is not None:\n",
        "      self.init_from_json(json_path)\n",
        "\n",
        "  def init_from_flags(self, FLAGS):\n",
        "    for key in self.keys:\n",
        "      setattr(self, key, getattr(FLAGS, key))\n",
        "\n",
        "  def init_from_json(self, json_path):\n",
        "    with tf.gfile.Open(json_path) as f:\n",
        "      json_data = json.load(f)\n",
        "      for key in self.keys:\n",
        "        setattr(self, key, json_data[key])\n",
        "\n",
        "  def to_json(self, json_path):\n",
        "    \"\"\"Save XLNetConfig to a json file.\"\"\"\n",
        "    json_data = {}\n",
        "    for key in self.keys:\n",
        "      json_data[key] = getattr(self, key)\n",
        "\n",
        "    json_dir = os.path.dirname(json_path)\n",
        "    if not tf.gfile.Exists(json_dir):\n",
        "      tf.gfile.MakeDirs(json_dir)\n",
        "    with tf.gfile.Open(json_path, \"w\") as f:\n",
        "      json.dump(json_data, f, indent=4, sort_keys=True)\n",
        "\n",
        "\n",
        "def create_run_config(is_training, is_finetune, FLAGS):\n",
        "  kwargs = dict(\n",
        "      is_training=is_training,\n",
        "      use_tpu=FLAGS.use_tpu,\n",
        "      use_bfloat16=FLAGS.use_bfloat16,\n",
        "      dropout=FLAGS.dropout,\n",
        "      dropatt=FLAGS.dropatt,\n",
        "      init=FLAGS.init,\n",
        "      init_range=FLAGS.init_range,\n",
        "      init_std=FLAGS.init_std,\n",
        "      clamp_len=FLAGS.clamp_len)\n",
        "\n",
        "  if not is_finetune:\n",
        "    kwargs.update(dict(\n",
        "        mem_len=FLAGS.mem_len,\n",
        "        reuse_len=FLAGS.reuse_len,\n",
        "        bi_data=FLAGS.bi_data,\n",
        "        clamp_len=FLAGS.clamp_len,\n",
        "        same_length=FLAGS.same_length))\n",
        "\n",
        "  return RunConfig(**kwargs)\n",
        "\n",
        "\n",
        "class RunConfig(object):\n",
        "  \"\"\"RunConfig contains hyperparameters that could be different\n",
        "  between pretraining and finetuning.\n",
        "  These hyperparameters can also be changed from run to run.\n",
        "  We store them separately from XLNetConfig for flexibility.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, is_training, use_tpu, use_bfloat16, dropout, dropatt,\n",
        "               init=\"normal\", init_range=0.1, init_std=0.02, mem_len=None,\n",
        "               reuse_len=None, bi_data=False, clamp_len=-1, same_length=False):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      is_training: bool, whether in training mode.\n",
        "      use_tpu: bool, whether TPUs are used.\n",
        "      use_bfloat16: bool, use bfloat16 instead of float32.\n",
        "      dropout: float, dropout rate.\n",
        "      dropatt: float, dropout rate on attention probabilities.\n",
        "      init: str, the initialization scheme, either \"normal\" or \"uniform\".\n",
        "      init_range: float, initialize the parameters with a uniform distribution\n",
        "        in [-init_range, init_range]. Only effective when init=\"uniform\".\n",
        "      init_std: float, initialize the parameters with a normal distribution\n",
        "        with mean 0 and stddev init_std. Only effective when init=\"normal\".\n",
        "      mem_len: int, the number of tokens to cache.\n",
        "      reuse_len: int, the number of tokens in the currect batch to be cached\n",
        "        and reused in the future.\n",
        "      bi_data: bool, whether to use bidirectional input pipeline.\n",
        "        Usually set to True during pretraining and False during finetuning.\n",
        "      clamp_len: int, clamp all relative distances larger than clamp_len.\n",
        "        -1 means no clamping.\n",
        "      same_length: bool, whether to use the same attention length for each token.\n",
        "    \"\"\"\n",
        "\n",
        "    self.init = init\n",
        "    self.init_range = init_range\n",
        "    self.init_std = init_std\n",
        "    self.is_training = is_training\n",
        "    self.dropout = dropout\n",
        "    self.dropatt = dropatt\n",
        "    self.use_tpu = use_tpu\n",
        "    self.use_bfloat16 = use_bfloat16\n",
        "    self.mem_len = mem_len\n",
        "    self.reuse_len = reuse_len\n",
        "    self.bi_data = bi_data\n",
        "    self.clamp_len = clamp_len\n",
        "    self.same_length = same_length\n",
        "\n",
        "\n",
        "class XLNetModel(object):\n",
        "  \"\"\"A wrapper of the XLNet model used during both pretraining and finetuning.\"\"\"\n",
        "\n",
        "  def __init__(self, xlnet_config, run_config, input_ids, seg_ids, input_mask,\n",
        "               mems=None, perm_mask=None, target_mapping=None, inp_q=None,\n",
        "               **kwargs):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      xlnet_config: XLNetConfig,\n",
        "      run_config: RunConfig,\n",
        "      input_ids: int32 Tensor in shape [len, bsz], the input token IDs.\n",
        "      seg_ids: int32 Tensor in shape [len, bsz], the input segment IDs.\n",
        "      input_mask: float32 Tensor in shape [len, bsz], the input mask.\n",
        "        0 for real tokens and 1 for padding.\n",
        "      mems: a list of float32 Tensors in shape [mem_len, bsz, d_model], memory\n",
        "        from previous batches. The length of the list equals n_layer.\n",
        "        If None, no memory is used.\n",
        "      perm_mask: float32 Tensor in shape [len, len, bsz].\n",
        "        If perm_mask[i, j, k] = 0, i attend to j in batch k;\n",
        "        if perm_mask[i, j, k] = 1, i does not attend to j in batch k.\n",
        "        If None, each position attends to all the others.\n",
        "      target_mapping: float32 Tensor in shape [num_predict, len, bsz].\n",
        "        If target_mapping[i, j, k] = 1, the i-th predict in batch k is\n",
        "        on the j-th token.\n",
        "        Only used during pretraining for partial prediction.\n",
        "        Set to None during finetuning.\n",
        "      inp_q: float32 Tensor in shape [len, bsz].\n",
        "        1 for tokens with losses and 0 for tokens without losses.\n",
        "        Only used during pretraining for two-stream attention.\n",
        "        Set to None during finetuning.\n",
        "    \"\"\"\n",
        "\n",
        "    initializer = _get_initializer(run_config)\n",
        "\n",
        "    tfm_args = dict(\n",
        "        n_token=xlnet_config.n_token,\n",
        "        initializer=initializer,\n",
        "        attn_type=\"bi\",\n",
        "        n_layer=xlnet_config.n_layer,\n",
        "        d_model=xlnet_config.d_model,\n",
        "        n_head=xlnet_config.n_head,\n",
        "        d_head=xlnet_config.d_head,\n",
        "        d_inner=xlnet_config.d_inner,\n",
        "        ff_activation=xlnet_config.ff_activation,\n",
        "        untie_r=xlnet_config.untie_r,\n",
        "\n",
        "        is_training=run_config.is_training,\n",
        "        use_bfloat16=run_config.use_bfloat16,\n",
        "        use_tpu=run_config.use_tpu,\n",
        "        dropout=run_config.dropout,\n",
        "        dropatt=run_config.dropatt,\n",
        "\n",
        "        mem_len=run_config.mem_len,\n",
        "        reuse_len=run_config.reuse_len,\n",
        "        bi_data=run_config.bi_data,\n",
        "        clamp_len=run_config.clamp_len,\n",
        "        same_length=run_config.same_length\n",
        "    )\n",
        "\n",
        "    input_args = dict(\n",
        "        inp_k=input_ids,\n",
        "        seg_id=seg_ids,\n",
        "        input_mask=input_mask,\n",
        "        mems=mems,\n",
        "        perm_mask=perm_mask,\n",
        "        target_mapping=target_mapping,\n",
        "        inp_q=inp_q)\n",
        "    tfm_args.update(input_args)\n",
        "\n",
        "    with tf.variable_scope(\"model\", reuse=tf.AUTO_REUSE):\n",
        "      (self.output, self.new_mems, self.lookup_table\n",
        "          ) = modeling.transformer_xl(**tfm_args)\n",
        "\n",
        "    self.input_mask = input_mask\n",
        "    self.initializer = initializer\n",
        "    self.xlnet_config = xlnet_config\n",
        "    self.run_config = run_config\n",
        "\n",
        "  def get_pooled_out(self, summary_type, use_summ_proj=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      summary_type: str, \"last\", \"first\", \"mean\", or \"attn\". The method\n",
        "        to pool the input to get a vector representation.\n",
        "      use_summ_proj: bool, whether to use a linear projection during pooling.\n",
        "    Returns:\n",
        "      float32 Tensor in shape [bsz, d_model], the pooled representation.\n",
        "    \"\"\"\n",
        "\n",
        "    xlnet_config = self.xlnet_config\n",
        "    run_config = self.run_config\n",
        "\n",
        "    with tf.variable_scope(\"model\", reuse=tf.AUTO_REUSE):\n",
        "      summary = modeling.summarize_sequence(\n",
        "          summary_type=summary_type,\n",
        "          hidden=self.output,\n",
        "          d_model=xlnet_config.d_model,\n",
        "          n_head=xlnet_config.n_head,\n",
        "          d_head=xlnet_config.d_head,\n",
        "          dropout=run_config.dropout,\n",
        "          dropatt=run_config.dropatt,\n",
        "          is_training=run_config.is_training,\n",
        "          input_mask=self.input_mask,\n",
        "          initializer=self.initializer,\n",
        "          use_proj=use_summ_proj)\n",
        "\n",
        "    return summary\n",
        "\n",
        "  def get_sequence_output(self):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      float32 Tensor in shape [len, bsz, d_model]. The last layer hidden\n",
        "      representation of XLNet.\n",
        "    \"\"\"\n",
        "\n",
        "    return self.output\n",
        "\n",
        "  def get_new_memory(self):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      list of float32 Tensors in shape [mem_len, bsz, d_model], the new\n",
        "      memory that concatenates the previous memory with the current input\n",
        "      representations.\n",
        "      The length of the list equals n_layer.\n",
        "    \"\"\"\n",
        "    return self.new_mems\n",
        "\n",
        "  def get_embedding_table(self):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      float32 Tensor in shape [n_token, d_model]. The embedding lookup table.\n",
        "      Used for tying embeddings between input and output layers.\n",
        "    \"\"\"\n",
        "    return self.lookup_table\n",
        "\n",
        "  def get_initializer(self):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      A tf initializer. Used to initialize variables in layers on top of XLNet.\n",
        "    \"\"\"\n",
        "    return self.initializer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns7QMSBHLXIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"doc.\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import tensorflow as tf\n",
        "#import modeling\n",
        "#import xlnet\n",
        "\n",
        "\n",
        "def construct_scalar_host_call(\n",
        "    monitor_dict,\n",
        "    model_dir,\n",
        "    prefix=\"\",\n",
        "    reduce_fn=None):\n",
        "  \"\"\"\n",
        "  Construct host calls to monitor training progress on TPUs.\n",
        "  \"\"\"\n",
        "\n",
        "  metric_names = list(monitor_dict.keys())\n",
        "\n",
        "  def host_call_fn(global_step, *args):\n",
        "    \"\"\"actual host call function.\"\"\"\n",
        "    step = global_step[0]\n",
        "    with tf.contrib.summary.create_file_writer(\n",
        "        logdir=model_dir, filename_suffix=\".host_call\").as_default():\n",
        "      with tf.contrib.summary.always_record_summaries():\n",
        "        for i, name in enumerate(metric_names):\n",
        "          if reduce_fn is None:\n",
        "            scalar = args[i][0]\n",
        "          else:\n",
        "            scalar = reduce_fn(args[i])\n",
        "          with tf.contrib.summary.record_summaries_every_n_global_steps(\n",
        "              100, global_step=step):\n",
        "            tf.contrib.summary.scalar(prefix + name, scalar, step=step)\n",
        "\n",
        "        return tf.contrib.summary.all_summary_ops()\n",
        "\n",
        "  global_step_tensor = tf.reshape(tf.train.get_or_create_global_step(), [1])\n",
        "  other_tensors = [tf.reshape(monitor_dict[key], [1]) for key in metric_names]\n",
        "\n",
        "  return host_call_fn, [global_step_tensor] + other_tensors\n",
        "\n",
        "\n",
        "def two_stream_loss(FLAGS, features, labels, mems, is_training):\n",
        "  \"\"\"Pretraining loss with two-stream attention Transformer-XL.\"\"\"\n",
        "\n",
        "  #### Unpack input\n",
        "  mem_name = \"mems\"\n",
        "  mems = mems.get(mem_name, None)\n",
        "\n",
        "  inp_k = tf.transpose(features[\"input_k\"], [1, 0])\n",
        "  inp_q = tf.transpose(features[\"input_q\"], [1, 0])\n",
        "\n",
        "  seg_id = tf.transpose(features[\"seg_id\"], [1, 0])\n",
        "\n",
        "  inp_mask = None\n",
        "  perm_mask = tf.transpose(features[\"perm_mask\"], [1, 2, 0])\n",
        "\n",
        "  if FLAGS.num_predict is not None:\n",
        "    # [num_predict x tgt_len x bsz]\n",
        "    target_mapping = tf.transpose(features[\"target_mapping\"], [1, 2, 0])\n",
        "  else:\n",
        "    target_mapping = None\n",
        "\n",
        "  # target for LM loss\n",
        "  tgt = tf.transpose(features[\"target\"], [1, 0])\n",
        "\n",
        "  # target mask for LM loss\n",
        "  tgt_mask = tf.transpose(features[\"target_mask\"], [1, 0])\n",
        "\n",
        "  # construct xlnet config and save to model_dir\n",
        "  xlnet_config = xlnet.XLNetConfig(FLAGS=FLAGS)\n",
        "  xlnet_config.to_json(os.path.join(FLAGS.model_dir, \"config.json\"))\n",
        "\n",
        "  # construct run config from FLAGS\n",
        "  run_config = xlnet.create_run_config(is_training, False, FLAGS)\n",
        "\n",
        "  xlnet_model = xlnet.XLNetModel(\n",
        "      xlnet_config=xlnet_config,\n",
        "      run_config=run_config,\n",
        "      input_ids=inp_k,\n",
        "      seg_ids=seg_id,\n",
        "      input_mask=inp_mask,\n",
        "      mems=mems,\n",
        "      perm_mask=perm_mask,\n",
        "      target_mapping=target_mapping,\n",
        "      inp_q=inp_q)\n",
        "\n",
        "  output = xlnet_model.get_sequence_output()\n",
        "  new_mems = {mem_name: xlnet_model.get_new_memory()}\n",
        "  lookup_table = xlnet_model.get_embedding_table()\n",
        "\n",
        "  initializer = xlnet_model.get_initializer()\n",
        "\n",
        "  with tf.variable_scope(\"model\", reuse=tf.AUTO_REUSE):\n",
        "    # LM loss\n",
        "    lm_loss = modeling.lm_loss(\n",
        "        hidden=output,\n",
        "        target=tgt,\n",
        "        n_token=xlnet_config.n_token,\n",
        "        d_model=xlnet_config.d_model,\n",
        "        initializer=initializer,\n",
        "        lookup_table=lookup_table,\n",
        "        tie_weight=True,\n",
        "        bi_data=run_config.bi_data,\n",
        "        use_tpu=run_config.use_tpu)\n",
        "\n",
        "  #### Quantity to monitor\n",
        "  monitor_dict = {}\n",
        "\n",
        "  if FLAGS.use_bfloat16:\n",
        "    tgt_mask = tf.cast(tgt_mask, tf.float32)\n",
        "    lm_loss = tf.cast(lm_loss, tf.float32)\n",
        "\n",
        "  total_loss = tf.reduce_sum(lm_loss * tgt_mask) / tf.reduce_sum(tgt_mask)\n",
        "  monitor_dict[\"total_loss\"] = total_loss\n",
        "\n",
        "  return total_loss, new_mems, monitor_dict\n",
        "\n",
        "\n",
        "def get_loss(FLAGS, features, labels, mems, is_training):\n",
        "  \"\"\"Pretraining loss with two-stream attention Transformer-XL.\"\"\"\n",
        "  if FLAGS.use_bfloat16:\n",
        "    with tf.tpu.bfloat16_scope():\n",
        "      return two_stream_loss(FLAGS, features, labels, mems, is_training)\n",
        "  else:\n",
        "    return two_stream_loss(FLAGS, features, labels, mems, is_training)\n",
        "\n",
        "\n",
        "def get_classification_loss(\n",
        "    FLAGS, features, n_class, is_training):\n",
        "  \"\"\"Loss for downstream classification tasks.\"\"\"\n",
        "\n",
        "  bsz_per_core = tf.shape(features[\"input_ids\"])[0]\n",
        "\n",
        "  inp = tf.transpose(features[\"input_ids\"], [1, 0])\n",
        "  seg_id = tf.transpose(features[\"segment_ids\"], [1, 0])\n",
        "  inp_mask = tf.transpose(features[\"input_mask\"], [1, 0])\n",
        "  label = tf.reshape(features[\"label_ids\"], [bsz_per_core])\n",
        "\n",
        "  xlnet_config = xlnet.XLNetConfig(json_path=FLAGS.model_config_path)\n",
        "  run_config = xlnet.create_run_config(is_training, True, FLAGS)\n",
        "\n",
        "  xlnet_model = xlnet.XLNetModel(\n",
        "      xlnet_config=xlnet_config,\n",
        "      run_config=run_config,\n",
        "      input_ids=inp,\n",
        "      seg_ids=seg_id,\n",
        "      input_mask=inp_mask)\n",
        "\n",
        "  summary = xlnet_model.get_pooled_out(FLAGS.summary_type, FLAGS.use_summ_proj)\n",
        "\n",
        "  with tf.variable_scope(\"model\", reuse=tf.AUTO_REUSE):\n",
        "\n",
        "    if FLAGS.cls_scope is not None and FLAGS.cls_scope:\n",
        "      cls_scope = \"classification_{}\".format(FLAGS.cls_scope)\n",
        "    else:\n",
        "      cls_scope = \"classification_{}\".format(FLAGS.task_name.lower())\n",
        "\n",
        "    per_example_loss, logits = modeling.classification_loss(\n",
        "        hidden=summary,\n",
        "        labels=label,\n",
        "        n_class=n_class,\n",
        "        initializer=xlnet_model.get_initializer(),\n",
        "        scope=cls_scope,\n",
        "        return_logits=True)\n",
        "\n",
        "    total_loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return total_loss, per_example_loss, logits\n",
        "\n",
        "\n",
        "def get_regression_loss(\n",
        "    FLAGS, features, is_training):\n",
        "  \"\"\"Loss for downstream regression tasks.\"\"\"\n",
        "\n",
        "  bsz_per_core = tf.shape(features[\"input_ids\"])[0]\n",
        "\n",
        "  inp = tf.transpose(features[\"input_ids\"], [1, 0])\n",
        "  seg_id = tf.transpose(features[\"segment_ids\"], [1, 0])\n",
        "  inp_mask = tf.transpose(features[\"input_mask\"], [1, 0])\n",
        "  label = tf.reshape(features[\"label_ids\"], [bsz_per_core])\n",
        "\n",
        "  xlnet_config = xlnet.XLNetConfig(json_path=FLAGS.model_config_path)\n",
        "  run_config = xlnet.create_run_config(is_training, True, FLAGS)\n",
        "\n",
        "  xlnet_model = xlnet.XLNetModel(\n",
        "      xlnet_config=xlnet_config,\n",
        "      run_config=run_config,\n",
        "      input_ids=inp,\n",
        "      seg_ids=seg_id,\n",
        "      input_mask=inp_mask)\n",
        "\n",
        "  summary = xlnet_model.get_pooled_out(FLAGS.summary_type, FLAGS.use_summ_proj)\n",
        "\n",
        "  with tf.variable_scope(\"model\", reuse=tf.AUTO_REUSE):\n",
        "    per_example_loss, logits = modeling.regression_loss(\n",
        "        hidden=summary,\n",
        "        labels=label,\n",
        "        initializer=xlnet_model.get_initializer(),\n",
        "        scope=\"regression_{}\".format(FLAGS.task_name.lower()),\n",
        "        return_logits=True)\n",
        "\n",
        "    total_loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return total_loss, per_example_loss, logits\n",
        "\n",
        "\n",
        "def get_qa_outputs(FLAGS, features, is_training):\n",
        "  \"\"\"Loss for downstream span-extraction QA tasks such as SQuAD.\"\"\"\n",
        "\n",
        "  inp = tf.transpose(features[\"input_ids\"], [1, 0])\n",
        "  seg_id = tf.transpose(features[\"segment_ids\"], [1, 0])\n",
        "  inp_mask = tf.transpose(features[\"input_mask\"], [1, 0])\n",
        "  cls_index = tf.reshape(features[\"cls_index\"], [-1])\n",
        "\n",
        "  seq_len = tf.shape(inp)[0]\n",
        "\n",
        "  xlnet_config = xlnet.XLNetConfig(json_path=FLAGS.model_config_path)\n",
        "  run_config = xlnet.create_run_config(is_training, True, FLAGS)\n",
        "\n",
        "  xlnet_model = xlnet.XLNetModel(\n",
        "      xlnet_config=xlnet_config,\n",
        "      run_config=run_config,\n",
        "      input_ids=inp,\n",
        "      seg_ids=seg_id,\n",
        "      input_mask=inp_mask)\n",
        "  output = xlnet_model.get_sequence_output()\n",
        "  initializer = xlnet_model.get_initializer()\n",
        "\n",
        "  return_dict = {}\n",
        "\n",
        "  # invalid position mask such as query and special symbols (PAD, SEP, CLS)\n",
        "  p_mask = features[\"p_mask\"]\n",
        "\n",
        "  # logit of the start position\n",
        "  with tf.variable_scope(\"start_logits\"):\n",
        "    start_logits = tf.layers.dense(\n",
        "        output,\n",
        "        1,\n",
        "        kernel_initializer=initializer)\n",
        "    start_logits = tf.transpose(tf.squeeze(start_logits, -1), [1, 0])\n",
        "    start_logits_masked = start_logits * (1 - p_mask) - 1e30 * p_mask\n",
        "    start_log_probs = tf.nn.log_softmax(start_logits_masked, -1)\n",
        "\n",
        "  # logit of the end position\n",
        "  with tf.variable_scope(\"end_logits\"):\n",
        "    if is_training:\n",
        "      # during training, compute the end logits based on the\n",
        "      # ground truth of the start position\n",
        "\n",
        "      start_positions = tf.reshape(features[\"start_positions\"], [-1])\n",
        "      start_index = tf.one_hot(start_positions, depth=seq_len, axis=-1,\n",
        "                               dtype=tf.float32)\n",
        "      start_features = tf.einsum(\"lbh,bl->bh\", output, start_index)\n",
        "      start_features = tf.tile(start_features[None], [seq_len, 1, 1])\n",
        "      end_logits = tf.layers.dense(\n",
        "          tf.concat([output, start_features], axis=-1), xlnet_config.d_model,\n",
        "          kernel_initializer=initializer, activation=tf.tanh, name=\"dense_0\")\n",
        "      end_logits = tf.contrib.layers.layer_norm(\n",
        "          end_logits, begin_norm_axis=-1)\n",
        "\n",
        "      end_logits = tf.layers.dense(\n",
        "          end_logits, 1,\n",
        "          kernel_initializer=initializer,\n",
        "          name=\"dense_1\")\n",
        "      end_logits = tf.transpose(tf.squeeze(end_logits, -1), [1, 0])\n",
        "      end_logits_masked = end_logits * (1 - p_mask) - 1e30 * p_mask\n",
        "      end_log_probs = tf.nn.log_softmax(end_logits_masked, -1)\n",
        "    else:\n",
        "      # during inference, compute the end logits based on beam search\n",
        "\n",
        "      start_top_log_probs, start_top_index = tf.nn.top_k(\n",
        "          start_log_probs, k=FLAGS.start_n_top)\n",
        "      start_index = tf.one_hot(start_top_index,\n",
        "                               depth=seq_len, axis=-1, dtype=tf.float32)\n",
        "      start_features = tf.einsum(\"lbh,bkl->bkh\", output, start_index)\n",
        "      end_input = tf.tile(output[:, :, None],\n",
        "                          [1, 1, FLAGS.start_n_top, 1])\n",
        "      start_features = tf.tile(start_features[None],\n",
        "                               [seq_len, 1, 1, 1])\n",
        "      end_input = tf.concat([end_input, start_features], axis=-1)\n",
        "      end_logits = tf.layers.dense(\n",
        "          end_input,\n",
        "          xlnet_config.d_model,\n",
        "          kernel_initializer=initializer,\n",
        "          activation=tf.tanh,\n",
        "          name=\"dense_0\")\n",
        "      end_logits = tf.contrib.layers.layer_norm(end_logits,\n",
        "                                                begin_norm_axis=-1)\n",
        "      end_logits = tf.layers.dense(\n",
        "          end_logits,\n",
        "          1,\n",
        "          kernel_initializer=initializer,\n",
        "          name=\"dense_1\")\n",
        "      end_logits = tf.reshape(end_logits, [seq_len, -1, FLAGS.start_n_top])\n",
        "      end_logits = tf.transpose(end_logits, [1, 2, 0])\n",
        "      end_logits_masked = end_logits * (\n",
        "          1 - p_mask[:, None]) - 1e30 * p_mask[:, None]\n",
        "      end_log_probs = tf.nn.log_softmax(end_logits_masked, -1)\n",
        "      end_top_log_probs, end_top_index = tf.nn.top_k(\n",
        "          end_log_probs, k=FLAGS.end_n_top)\n",
        "      end_top_log_probs = tf.reshape(\n",
        "          end_top_log_probs,\n",
        "          [-1, FLAGS.start_n_top * FLAGS.end_n_top])\n",
        "      end_top_index = tf.reshape(\n",
        "          end_top_index,\n",
        "          [-1, FLAGS.start_n_top * FLAGS.end_n_top])\n",
        "\n",
        "  if is_training:\n",
        "    return_dict[\"start_log_probs\"] = start_log_probs\n",
        "    return_dict[\"end_log_probs\"] = end_log_probs\n",
        "  else:\n",
        "    return_dict[\"start_top_log_probs\"] = start_top_log_probs\n",
        "    return_dict[\"start_top_index\"] = start_top_index\n",
        "    return_dict[\"end_top_log_probs\"] = end_top_log_probs\n",
        "    return_dict[\"end_top_index\"] = end_top_index\n",
        "\n",
        "  # an additional layer to predict answerability\n",
        "  with tf.variable_scope(\"answer_class\"):\n",
        "    # get the representation of CLS\n",
        "    cls_index = tf.one_hot(cls_index, seq_len, axis=-1, dtype=tf.float32)\n",
        "    cls_feature = tf.einsum(\"lbh,bl->bh\", output, cls_index)\n",
        "\n",
        "    # get the representation of START\n",
        "    start_p = tf.nn.softmax(start_logits_masked, axis=-1,\n",
        "                            name=\"softmax_start\")\n",
        "    start_feature = tf.einsum(\"lbh,bl->bh\", output, start_p)\n",
        "\n",
        "    # note(zhiliny): no dependency on end_feature so that we can obtain\n",
        "    # one single `cls_logits` for each sample\n",
        "    ans_feature = tf.concat([start_feature, cls_feature], -1)\n",
        "    ans_feature = tf.layers.dense(\n",
        "        ans_feature,\n",
        "        xlnet_config.d_model,\n",
        "        activation=tf.tanh,\n",
        "        kernel_initializer=initializer, name=\"dense_0\")\n",
        "    ans_feature = tf.layers.dropout(ans_feature, FLAGS.dropout,\n",
        "                                    training=is_training)\n",
        "    cls_logits = tf.layers.dense(\n",
        "        ans_feature,\n",
        "        1,\n",
        "        kernel_initializer=initializer,\n",
        "        name=\"dense_1\",\n",
        "        use_bias=False)\n",
        "    cls_logits = tf.squeeze(cls_logits, -1)\n",
        "\n",
        "    return_dict[\"cls_logits\"] = cls_logits\n",
        "\n",
        "  return return_dict\n",
        "\n",
        "\n",
        "def get_race_loss(FLAGS, features, is_training):\n",
        "  \"\"\"Loss for downstream multi-choice QA tasks such as RACE.\"\"\"\n",
        "\n",
        "  bsz_per_core = tf.shape(features[\"input_ids\"])[0]\n",
        "\n",
        "  def _transform_features(feature):\n",
        "    out = tf.reshape(feature, [bsz_per_core, 4, -1])\n",
        "    out = tf.transpose(out, [2, 0, 1])\n",
        "    out = tf.reshape(out, [-1, bsz_per_core * 4])\n",
        "    return out\n",
        "\n",
        "  inp = _transform_features(features[\"input_ids\"])\n",
        "  seg_id = _transform_features(features[\"segment_ids\"])\n",
        "  inp_mask = _transform_features(features[\"input_mask\"])\n",
        "  label = tf.reshape(features[\"label_ids\"], [bsz_per_core])\n",
        "\n",
        "  xlnet_config = xlnet.XLNetConfig(json_path=FLAGS.model_config_path)\n",
        "  run_config = xlnet.create_run_config(is_training, True, FLAGS)\n",
        "\n",
        "  xlnet_model = xlnet.XLNetModel(\n",
        "      xlnet_config=xlnet_config,\n",
        "      run_config=run_config,\n",
        "      input_ids=inp,\n",
        "      seg_ids=seg_id,\n",
        "      input_mask=inp_mask)\n",
        "  summary = xlnet_model.get_pooled_out(FLAGS.summary_type, FLAGS.use_summ_proj)\n",
        "\n",
        "  with tf.variable_scope(\"logits\"):\n",
        "    logits = tf.layers.dense(summary, 1,\n",
        "        kernel_initializer=xlnet_model.get_initializer())\n",
        "    logits = tf.reshape(logits, [bsz_per_core, 4])\n",
        "\n",
        "    one_hot_target = tf.one_hot(label, 4)\n",
        "    per_example_loss = -tf.reduce_sum(\n",
        "        tf.nn.log_softmax(logits) * one_hot_target, -1)\n",
        "    total_loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "  return total_loss, per_example_loss, logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvzxgfB7K2dN",
        "colab_type": "code",
        "outputId": "f11b1b4e-b2fd-43dd-8da1-af89d2ef1fba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "source": [
        "# coding=utf-8\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import unicodedata\n",
        "import six\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "SPIECE_UNDERLINE = '▁'\n",
        "\n",
        "\n",
        "def printable_text(text):\n",
        "  \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n",
        "\n",
        "  # These functions want `str` for both Python2 and Python3, but in one case\n",
        "  # it's a Unicode string and in the other it's a byte string.\n",
        "  if six.PY3:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, bytes):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  elif six.PY2:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, unicode):\n",
        "      return text.encode(\"utf-8\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  else:\n",
        "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def print_(*args):\n",
        "  new_args = []\n",
        "  for arg in args:\n",
        "    if isinstance(arg, list):\n",
        "      s = [printable_text(i) for i in arg]\n",
        "      s = ' '.join(s)\n",
        "      new_args.append(s)\n",
        "    else:\n",
        "      new_args.append(printable_text(arg))\n",
        "  print(*new_args)\n",
        "\n",
        "\n",
        "def preprocess_text(inputs, lower=False, remove_space=True, keep_accents=False):\n",
        "  if remove_space:\n",
        "    outputs = ' '.join(inputs.strip().split())\n",
        "  else:\n",
        "    outputs = inputs\n",
        "  outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n",
        "\n",
        "  if six.PY2 and isinstance(outputs, str):\n",
        "    outputs = outputs.decode('utf-8')\n",
        "\n",
        "  if not keep_accents:\n",
        "    outputs = unicodedata.normalize('NFKD', outputs)\n",
        "    outputs = ''.join([c for c in outputs if not unicodedata.combining(c)])\n",
        "  if lower:\n",
        "    outputs = outputs.lower()\n",
        "\n",
        "  return outputs\n",
        "\n",
        "\n",
        "def encode_pieces(sp_model, text, return_unicode=True, sample=False):\n",
        "  # return_unicode is used only for py2\n",
        "\n",
        "  # note(zhiliny): in some systems, sentencepiece only accepts str for py2\n",
        "  if six.PY2 and isinstance(text, unicode):\n",
        "    text = text.encode('utf-8')\n",
        "\n",
        "  if not sample:\n",
        "    pieces = sp_model.EncodeAsPieces(text)\n",
        "  else:\n",
        "    pieces = sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n",
        "  new_pieces = []\n",
        "  for piece in pieces:\n",
        "    if len(piece) > 1 and piece[-1] == ',' and piece[-2].isdigit():\n",
        "      cur_pieces = sp_model.EncodeAsPieces(\n",
        "          piece[:-1].replace(SPIECE_UNDERLINE, ''))\n",
        "      if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
        "        if len(cur_pieces[0]) == 1:\n",
        "          cur_pieces = cur_pieces[1:]\n",
        "        else:\n",
        "          cur_pieces[0] = cur_pieces[0][1:]\n",
        "      cur_pieces.append(piece[-1])\n",
        "      new_pieces.extend(cur_pieces)\n",
        "    else:\n",
        "      new_pieces.append(piece)\n",
        "\n",
        "  # note(zhiliny): convert back to unicode for py2\n",
        "  if six.PY2 and return_unicode:\n",
        "    ret_pieces = []\n",
        "    for piece in new_pieces:\n",
        "      if isinstance(piece, str):\n",
        "        piece = piece.decode('utf-8')\n",
        "      ret_pieces.append(piece)\n",
        "    new_pieces = ret_pieces\n",
        "\n",
        "  return new_pieces\n",
        "\n",
        "\n",
        "def encode_ids(sp_model, text, sample=False):\n",
        "  pieces = encode_pieces(sp_model, text, return_unicode=False, sample=sample)\n",
        "  ids = [sp_model.PieceToId(piece) for piece in pieces]\n",
        "  return ids\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  import sentencepiece as spm\n",
        "\n",
        "  sp = spm.SentencePieceProcessor()\n",
        "  sp.load('sp10m.uncased.v3.model')\n",
        "\n",
        "  print_(u'I was born in 2000, and this is falsé.')\n",
        "  print_(u'ORIGINAL', sp.EncodeAsPieces(u'I was born in 2000, and this is falsé.'))\n",
        "  print_(u'OURS', encode_pieces(sp, u'I was born in 2000, and this is falsé.'))\n",
        "  print(encode_ids(sp, u'I was born in 2000, and this is falsé.'))\n",
        "  print_('')\n",
        "  prepro_func = partial(preprocess_text, lower=True)\n",
        "  print_(prepro_func('I was born in 2000, and this is falsé.'))\n",
        "  print_('ORIGINAL', sp.EncodeAsPieces(prepro_func('I was born in 2000, and this is falsé.')))\n",
        "  print_('OURS', encode_pieces(sp, prepro_func('I was born in 2000, and this is falsé.')))\n",
        "  print(encode_ids(sp, prepro_func('I was born in 2000, and this is falsé.')))\n",
        "  print_('')\n",
        "  print_('I was born in 2000, and this is falsé.')\n",
        "  print_('ORIGINAL', sp.EncodeAsPieces('I was born in 2000, and this is falsé.'))\n",
        "  print_('OURS', encode_pieces(sp, 'I was born in 2000, and this is falsé.'))\n",
        "  print(encode_ids(sp, 'I was born in 2000, and this is falsé.'))\n",
        "  print_('')\n",
        "  print_('I was born in 92000, and this is falsé.')\n",
        "  print_('ORIGINAL', sp.EncodeAsPieces('I was born in 92000, and this is falsé.'))\n",
        "  print_('OURS', encode_pieces(sp, 'I was born in 92000, and this is falsé.'))\n",
        "  print(encode_ids(sp, 'I was born in 92000, and this is falsé.'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-86c32eb4d18b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m   \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m   \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sp10m.uncased.v3.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0mprint_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'I was born in 2000, and this is falsé.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sentencepiece.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_from_serialized_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Not found: \"sp10m.uncased.v3.model\": No such file or directory Error #2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujX2LaIfJ39a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}