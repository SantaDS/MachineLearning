{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepDream Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Inception v5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#\n",
    "# Functions for downloading and extracting data-files from the internet.\n",
    "#\n",
    "# Implemented in Python 3.5\n",
    "#\n",
    "########################################################################\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import zipfile\n",
    "\n",
    "def _print_download_progress(count, block_size, total_size):\n",
    "    \"\"\"\n",
    "    Function used for printing the download progress.\n",
    "    Used as a call-back function in maybe_download_and_extract().\n",
    "    \"\"\"\n",
    "\n",
    "    # Percentage completion.\n",
    "    pct_complete = float(count * block_size) / total_size\n",
    "\n",
    "    # Status-message. Note the \\r which means the line should overwrite itself.\n",
    "    msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n",
    "\n",
    "    # Print it.\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def maybe_download_and_extract(url, download_dir):\n",
    "    \"\"\"\n",
    "    Download and extract the data if it doesn't already exist.\n",
    "    Assumes the url is a tar-ball file.\n",
    "\n",
    "    :param url:\n",
    "        Internet URL for the tar-file to download.\n",
    "        Example: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "\n",
    "    :param download_dir:\n",
    "        Directory where the downloaded file is saved.\n",
    "        Example: \"data/CIFAR-10/\"\n",
    "\n",
    "    :return:\n",
    "        Nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filename for saving the file downloaded from the internet.\n",
    "    # Use the filename from the URL and add it to the download_dir.\n",
    "    filename = url.split('/')[-1]\n",
    "    file_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    # Check if the file already exists.\n",
    "    # If it exists then we assume it has also been extracted,\n",
    "    # otherwise we need to download and extract it now.\n",
    "    if not os.path.exists(file_path):\n",
    "        # Check if the download directory exists, otherwise create it.\n",
    "        if not os.path.exists(download_dir):\n",
    "            os.makedirs(download_dir)\n",
    "\n",
    "        # Download the file from the internet.\n",
    "        file_path, _ = urllib.request.urlretrieve(url=url,\n",
    "                                                  filename=file_path,\n",
    "                                                  reporthook=_print_download_progress)\n",
    "\n",
    "        print()\n",
    "        print(\"Download finished. Extracting files.\")\n",
    "\n",
    "        if file_path.endswith(\".zip\"):\n",
    "            # Unpack the zip-file.\n",
    "            zipfile.ZipFile(file=file_path, mode=\"r\").extractall(download_dir)\n",
    "        elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
    "            # Unpack the tar-ball.\n",
    "            tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n",
    "\n",
    "        print(\"Done.\")\n",
    "    else:\n",
    "        print(\"Data has apparently already been downloaded and unpacked.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Gradients of given tensor for input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#\n",
    "# The Inception Model 5h for TensorFlow.\n",
    "#\n",
    "# This variant of the Inception model is easier to use for DeepDream\n",
    "# and other imaging techniques. This is because it allows the input\n",
    "# image to be any size, and the optimized images are also prettier.\n",
    "#\n",
    "# It is unclear which Inception model this implements because the\n",
    "# Google developers have (as usual) neglected to document it.\n",
    "# It is dubbed the 5h-model because that is the name of the zip-file,\n",
    "# but it is apparently simpler than the v.3 model.\n",
    "#\n",
    "# Implemented in Python 3.5 with TensorFlow v0.11.0rc0\n",
    "#\n",
    "########################################################################\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#import download\n",
    "import os\n",
    "\n",
    "########################################################################\n",
    "# Various directories and file-names.\n",
    "\n",
    "# Internet URL for the tar-file with the Inception model.\n",
    "data_url = \"http://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip\"\n",
    "\n",
    "# Directory to store the downloaded data.\n",
    "data_dir = \"inception/5h/\"\n",
    "\n",
    "# File containing the TensorFlow graph definition. (Downloaded)\n",
    "path_graph_def = \"tensorflow_inception_graph.pb\"\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def maybe_download():\n",
    "    \"\"\"\n",
    "    Downloading the Inception model from the internet if it does not already\n",
    "    exist in the data_dir. The file is about 50 MB.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Downloading Inception 5h Model ...\")\n",
    "    download.maybe_download_and_extract(url=data_url, download_dir=data_dir)\n",
    "\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "class Inception5h:\n",
    "    \"\"\"\n",
    "    The Inception model is a Deep Neural Network which has already been\n",
    "    trained for classifying images into 1000 different categories.\n",
    "\n",
    "    When you create a new instance of this class, the Inception model\n",
    "    will be loaded and can be used immediately without training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Name of the tensor for feeding the input image.\n",
    "    tensor_name_input_image = \"input:0\"\n",
    "\n",
    "    # Names for some of the commonly used layers in the Inception model.\n",
    "    layer_names = ['conv2d0', 'conv2d1', 'conv2d2',\n",
    "                   'mixed3a', 'mixed3b',\n",
    "                   'mixed4a', 'mixed4b', 'mixed4c', 'mixed4d', 'mixed4e',\n",
    "                   'mixed5a', 'mixed5b']\n",
    "\n",
    "    def __init__(self):\n",
    "        # Now loading the Inception model from file. The way TensorFlow\n",
    "        # does this is confusing and requires several steps.\n",
    "\n",
    "        # Create a new TensorFlow computational graph.\n",
    "        self.graph = tf.Graph()\n",
    "\n",
    "        # Set the new graph as the default.\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            # TensorFlow graphs are saved to disk as so-called Protocol Buffers\n",
    "            # aka. proto-bufs which is a file-format that works on multiple\n",
    "            # platforms. In this case it is saved as a binary file.\n",
    "\n",
    "            # Open the graph-def file for binary reading.\n",
    "            path = os.path.join(data_dir, path_graph_def)\n",
    "            with tf.gfile.FastGFile(path, 'rb') as file:\n",
    "                # The graph-def is a saved copy of a TensorFlow graph.\n",
    "                # First we need to create an empty graph-def.\n",
    "                graph_def = tf.GraphDef()\n",
    "\n",
    "                # Then loading the proto-buf file into the graph-def.\n",
    "                graph_def.ParseFromString(file.read())\n",
    "\n",
    "                # Finally importing the graph-def to the default TensorFlow graph.\n",
    "                tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "                # Now self.graph holds the Inception model from the proto-buf file.\n",
    "\n",
    "            # Get a reference to the tensor for inputting images to the graph.\n",
    "            self.input = self.graph.get_tensor_by_name(self.tensor_name_input_image)\n",
    "\n",
    "            # Get references to the tensors for the commonly used layers.\n",
    "            self.layer_tensors = [self.graph.get_tensor_by_name(name + \":0\") for name in self.layer_names]\n",
    "\n",
    "    def create_feed_dict(self, image=None):\n",
    "        \"\"\"\n",
    "        Create and return a feed-dict with an image.\n",
    "\n",
    "        :param image:\n",
    "            The input image is a 3-dim array which is already decoded.\n",
    "            The pixels MUST be values between 0 and 255 (float or int).\n",
    "\n",
    "        :return:\n",
    "            Dict for feeding to the Inception graph in TensorFlow.\n",
    "        \"\"\"\n",
    "\n",
    "        # Expand 3-dim array to 4-dim by prepending an 'empty' dimension.\n",
    "        # This is because we are only feeding a single image, but the\n",
    "        # Inception model was built to take multiple images as input.\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "\n",
    "        # Image is passed in as a 3-dim array of raw pixel-values.\n",
    "        feed_dict = {self.tensor_name_input_image: image}\n",
    "\n",
    "        return feed_dict\n",
    "\n",
    "    def get_gradient(self, tensor):\n",
    "        \"\"\"\n",
    "        Get the gradient of the given tensor with respect to\n",
    "        the input image. This allows us to modify the input\n",
    "        image so as to maximize the given tensor.\n",
    "\n",
    "        For use in e.g. DeepDream and Visual Analysis.\n",
    "\n",
    "        :param tensor:\n",
    "            The tensor whose value we want to maximize\n",
    "            by changing the input image.\n",
    "\n",
    "        :return:\n",
    "            Gradient for the tensor with regard to the input image.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the graph as default so we can add operations to it.\n",
    "        with self.graph.as_default():\n",
    "            # Square the tensor-values.\n",
    "            # You can try and remove this to see the effect.\n",
    "            tensor = tf.square(tensor)\n",
    "\n",
    "            # Average the tensor so we get a single scalar value.\n",
    "            tensor_mean = tf.reduce_mean(tensor)\n",
    "\n",
    "            # Use TensorFlow to automatically create a mathematical\n",
    "            # formula for the gradient using the chain-rule of\n",
    "            # differentiation.\n",
    "            gradient = tf.gradients(tensor_mean, self.input)[0]\n",
    "\n",
    "        return gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing and Reshaping input image to amplify abstract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "# Image manipulation.\n",
    "import PIL.Image\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "#import inception5h\n",
    "\n",
    "\n",
    "model = Inception5h()\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "session = tf.Session(graph=model.graph,\n",
    "                     config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "\n",
    "def load_image(filename):\n",
    "    image = PIL.Image.open(filename)\n",
    "    return np.float32(image)\n",
    "\n",
    "\n",
    "def save_image(image, filename):\n",
    "    # Ensure the pixel-values are between 0 and 255.\n",
    "    image = np.clip(image, 0.0, 255.0)\n",
    "    # Convert to bytes.\n",
    "    image = image.astype(np.uint8)\n",
    "    # Write the image-file in jpeg-format.\n",
    "    with open(filename, 'wb') as file:\n",
    "        PIL.Image.fromarray(image).save(file, 'jpeg')\n",
    "\n",
    "\n",
    "def plot_image(image):\n",
    "    # Assume the pixel-values are scaled between 0 and 255.\n",
    "    if False:\n",
    "        # Convert the pixel-values to the range between 0.0 and 1.0\n",
    "        image = np.clip(image/255.0, 0.0, 1.0)\n",
    "        # Plot using matplotlib.\n",
    "        plt.imshow(image, interpolation='lanczos')\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Ensure the pixel-values are between 0 and 255.\n",
    "        image = np.clip(image, 0.0, 255.0)\n",
    "        # Convert pixels to bytes.\n",
    "        image = image.astype(np.uint8)\n",
    "\n",
    "        # Convert to a PIL-image and display it.\n",
    "        plt.imshow(image, interpolation='lanczos')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def normalize_image(x):\n",
    "    # Get the min and max values for all pixels in the input.\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "\n",
    "    # Normalize so all values are between 0.0 and 1.0\n",
    "    x_norm = (x - x_min) / (x_max - x_min)\n",
    "    return x_norm\n",
    "\n",
    "\n",
    "def plot_gradient(gradient):\n",
    "    # Normalize the gradient so it is between 0.0 and 1.0\n",
    "    gradient_normalized = normalize_image(gradient)\n",
    "    # Plot the normalized gradient.\n",
    "    plt.imshow(gradient_normalized, interpolation='bilinear')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def resize_image(image, size=None, factor=None):\n",
    "    # If a rescaling-factor is provided then use it.\n",
    "    if factor is not None:\n",
    "        # Scale the numpy array's shape for height and width.\n",
    "        size = np.array(image.shape[0:2]) * factor\n",
    "        # The size is floating-point because it was scaled.\n",
    "        # PIL requires the size to be integers.\n",
    "        size = size.astype(int)\n",
    "    else:\n",
    "        # Ensure the size has length 2.\n",
    "        size = size[0:2]\n",
    "    # The height and width is reversed in numpy vs. PIL.\n",
    "    size = tuple(reversed(size))\n",
    "\n",
    "    # Ensure the pixel-values are between 0 and 255.\n",
    "    img = np.clip(image, 0.0, 255.0)\n",
    "    # Convert the pixels to 8-bit bytes.\n",
    "    img = img.astype(np.uint8)\n",
    "    # Create PIL-object from numpy array.\n",
    "    img = PIL.Image.fromarray(img)\n",
    "    # Resize the image.\n",
    "    img_resized = img.resize(size, PIL.Image.LANCZOS)\n",
    "    # Convert 8-bit pixel values back to floating-point.\n",
    "    img_resized = np.float32(img_resized)\n",
    "\n",
    "    return img_resized\n",
    "\n",
    "\n",
    "def get_tile_size(num_pixels, tile_size=400):\n",
    "    \"\"\"\n",
    "    num_pixels is the number of pixels in a dimension of the image.\n",
    "    tile_size is the desired tile-size.\n",
    "    \"\"\"\n",
    "\n",
    "    # How many times can we repeat a tile of the desired size.\n",
    "    num_tiles = int(round(num_pixels / tile_size))\n",
    "    # Ensure that there is at least 1 tile.\n",
    "    num_tiles = max(1, num_tiles)\n",
    "    # The actual tile-size.\n",
    "    actual_tile_size = math.ceil(num_pixels / num_tiles)\n",
    "    return actual_tile_size\n",
    "\n",
    "\n",
    "def tiled_gradient(gradient, image, tile_size=400):\n",
    "    # Allocate an array for the gradient of the entire image.\n",
    "    grad = np.zeros_like(image)\n",
    "\n",
    "    # Number of pixels for the x- and y-axes.\n",
    "    x_max, y_max, _ = image.shape\n",
    "\n",
    "    # Tile-size for the x-axis.\n",
    "    x_tile_size = get_tile_size(num_pixels=x_max, tile_size=tile_size)\n",
    "    # 1/4 of the tile-size.\n",
    "    x_tile_size4 = x_tile_size // 4\n",
    "\n",
    "    # Tile-size for the y-axis.\n",
    "    y_tile_size = get_tile_size(num_pixels=y_max, tile_size=tile_size)\n",
    "    # 1/4 of the tile-size\n",
    "    y_tile_size4 = y_tile_size // 4\n",
    "\n",
    "    # Random start-position for the tiles on the x-axis.\n",
    "    # The random value is between -3/4 and -1/4 of the tile-size.\n",
    "    # This is so the border-tiles are at least 1/4 of the tile-size,\n",
    "    # otherwise the tiles may be too small which creates noisy gradients.\n",
    "    x_start = random.randint(-3*x_tile_size4, -x_tile_size4)\n",
    "\n",
    "    while x_start < x_max:\n",
    "        # End-position for the current tile.\n",
    "        x_end = x_start + x_tile_size\n",
    "        # Ensure the tile's start- and end-positions are valid.\n",
    "        x_start_lim = max(x_start, 0)\n",
    "        x_end_lim = min(x_end, x_max)\n",
    "\n",
    "        # Random start-position for the tiles on the y-axis.\n",
    "        # The random value is between -3/4 and -1/4 of the tile-size.\n",
    "        y_start = random.randint(-3*y_tile_size4, -y_tile_size4)\n",
    "\n",
    "        while y_start < y_max:\n",
    "            # End-position for the current tile.\n",
    "            y_end = y_start + y_tile_size\n",
    "\n",
    "            # Ensure the tile's start- and end-positions are valid.\n",
    "            y_start_lim = max(y_start, 0)\n",
    "            y_end_lim = min(y_end, y_max)\n",
    "\n",
    "            # Get the image-tile.\n",
    "            img_tile = image[x_start_lim:x_end_lim,\n",
    "                             y_start_lim:y_end_lim, :]\n",
    "\n",
    "            # Create a feed-dict with the image-tile.\n",
    "            feed_dict = model.create_feed_dict(image=img_tile)\n",
    "\n",
    "            # Use TensorFlow to calculate the gradient-value.\n",
    "            g = session.run(gradient, feed_dict=feed_dict)\n",
    "\n",
    "            # Normalize the gradient for the tile. This is\n",
    "            # necessary because the tiles may have very different\n",
    "            # values. Normalizing gives a more coherent gradient.\n",
    "            g /= (np.std(g) + 1e-8)\n",
    "\n",
    "            # Store the tile's gradient at the appropriate location.\n",
    "            grad[x_start_lim:x_end_lim,\n",
    "                 y_start_lim:y_end_lim, :] = g\n",
    "            # Advance the start-position for the y-axis.\n",
    "            y_start = y_end\n",
    "\n",
    "        # Advance the start-position for the x-axis.\n",
    "        x_start = x_end\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "def optimize_image(layer_tensor, image,\n",
    "                   num_iterations=10, step_size=3.0, tile_size=400,\n",
    "                   show_gradient=False):\n",
    "    \"\"\"\n",
    "    Using gradient ascent to optimize an image so it maximizes the\n",
    "    mean value of the given layer_tensor.\n",
    "\n",
    "    Parameters:\n",
    "    layer_tensor: Reference to a tensor that will be maximized.\n",
    "    image: Input image used as the starting point.\n",
    "    num_iterations: Number of optimization iterations to perform.\n",
    "    step_size: Scale for each step of the gradient ascent.\n",
    "    tile_size: Size of the tiles when calculating the gradient.\n",
    "    show_gradient: Plot the gradient in each iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy the image so we don't overwrite the original image.\n",
    "    img = image.copy()\n",
    "\n",
    "    print(\"Processing image: \")\n",
    "\n",
    "    # Using TensorFlow to get the mathematical function for the\n",
    "    # gradient of the given layer-tensor with regard to the\n",
    "    # input image. This may cause TensorFlow to add the same\n",
    "    # math-expressions to the graph each time this function is called.\n",
    "    \n",
    "    gradient = model.get_gradient(layer_tensor)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Calculating the value of the gradient.\n",
    "        # This tells us how to change the image so as to\n",
    "        # maximize the mean of the given layer-tensor.\n",
    "        grad = tiled_gradient(gradient=gradient, image=img)\n",
    "\n",
    "        # Bluring the gradient with different amounts and add\n",
    "        # them together. The blur amount is also increased\n",
    "        # during the optimization. This was found to give\n",
    "        # nice, smooth images. You can try and change the formulas.\n",
    "        # The blur-amount is called sigma (0=no blur, 1=low blur, etc.)\n",
    "        # We could call gaussian_filter(grad, sigma=(sigma, sigma, 0.0))\n",
    "        # which would not blur the colour-channel. This tends to\n",
    "        # give psychadelic / pastel colours in the resulting images.\n",
    "        # When the colour-channel is also blurred the colours of the\n",
    "        # input image are mostly retained in the output image.\n",
    "        sigma = (i * 4.0) / num_iterations + 0.5\n",
    "        grad_smooth1 = gaussian_filter(grad, sigma=sigma)\n",
    "        grad_smooth2 = gaussian_filter(grad, sigma=sigma*2)\n",
    "        grad_smooth3 = gaussian_filter(grad, sigma=sigma*0.5)\n",
    "        grad = (grad_smooth1 + grad_smooth2 + grad_smooth3)\n",
    "\n",
    "        # Scale the step-size according to the gradient-values.\n",
    "        # This may not be necessary because the tiled-gradient\n",
    "        # is already normalized.\n",
    "        step_size_scaled = step_size / (np.std(grad) + 1e-8)\n",
    "\n",
    "        # Update the image by following the gradient.\n",
    "        img += grad * step_size_scaled\n",
    "\n",
    "        if show_gradient:\n",
    "            # Print statistics for the gradient.\n",
    "            msg = \"Gradient min: {0:>9.6f}, max: {1:>9.6f}, stepsize: {2:>9.2f}\"\n",
    "            print(msg.format(grad.min(), grad.max(), step_size_scaled))\n",
    "\n",
    "            # Plot the gradient.\n",
    "            plot_gradient(grad)\n",
    "        else:\n",
    "            # Otherwise show a little progress-indicator.\n",
    "            print(\". \", end=\"\")\n",
    "    return img\n",
    "\n",
    "\n",
    "def recursive_optimize(layer_tensor, image,\n",
    "                       num_repeats=4, rescale_factor=0.7, blend=0.2,\n",
    "                       num_iterations=10, step_size=3.0,\n",
    "                       tile_size=400):\n",
    "    \"\"\"\n",
    "    Recursively blur and downscale the input image.\n",
    "    Each downscaled image is run through the optimize_image()\n",
    "    function to amplify the patterns that the Inception model sees.\n",
    "\n",
    "    Parameters:\n",
    "    image: Input image used as the starting point.\n",
    "    rescale_factor: Downscaling factor for the image.\n",
    "    num_repeats: Number of times to downscale the image.\n",
    "    blend: Factor for blending the original and processed images.\n",
    "\n",
    "    Parameters passed to optimize_image():\n",
    "    layer_tensor: Reference to a tensor that will be maximized.\n",
    "    num_iterations: Number of optimization iterations to perform.\n",
    "    step_size: Scale for each step of the gradient ascent.\n",
    "    tile_size: Size of the tiles when calculating the gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    # Do a recursive step\n",
    "    if num_repeats > 0:\n",
    "        # Blur the input image to prevent artifacts when downscaling.\n",
    "        # The blur amount is controlled by sigma. Note that the\n",
    "        # colour-channel is not blurred as it would make the image gray.\n",
    "        sigma = 0.5\n",
    "        img_blur = gaussian_filter(image, sigma=(sigma, sigma, 0.0))\n",
    "\n",
    "        # Downscale the image.\n",
    "        img_downscaled = resize_image(image=img_blur,\n",
    "                                      factor=rescale_factor)\n",
    "\n",
    "        # Recursive call to this function.\n",
    "        # Subtract one from num_repeats and use the downscaled image.\n",
    "        img_result = recursive_optimize(layer_tensor=layer_tensor,\n",
    "                                        image=img_downscaled,\n",
    "                                        num_repeats=num_repeats-1,\n",
    "                                        rescale_factor=rescale_factor,\n",
    "                                        blend=blend,\n",
    "                                        num_iterations=num_iterations,\n",
    "                                        step_size=step_size,\n",
    "                                        tile_size=tile_size)\n",
    "\n",
    "        # Upscale the resulting image back to its original size.\n",
    "        img_upscaled = resize_image(image=img_result, size=image.shape)\n",
    "\n",
    "        # Blend the original and processed images.\n",
    "        image = blend * image + (1.0 - blend) * img_upscaled\n",
    "\n",
    "    print(\"Recursive level:\", num_repeats)\n",
    "\n",
    "    # Process the image using the DeepDream algorithm.\n",
    "    img_result = optimize_image(layer_tensor=layer_tensor,\n",
    "                                image=image,\n",
    "                                num_iterations=num_iterations,\n",
    "                                step_size=step_size,\n",
    "                                tile_size=tile_size)\n",
    "\n",
    "    return img_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the intermediate images for every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepdreamer import model, load_image, recursive_optimize\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "layer_tensor = model.layer_tensors[5]\n",
    "dream_name = 'northern_light'\n",
    "x_size = 756\n",
    "y_size = 473\n",
    "\n",
    "created_count = 0\n",
    "max_count = 105\n",
    "for i in range(1234567):\n",
    "    if os.path.isfile('dream/{}/image_{}.jpg'.format(dream_name,i+1)):\n",
    "        print('{} already exists, continueing along...'.format(i+1))\n",
    "    else:\n",
    "        img_result = load_image(filename='dream/{}/image_{}.jpg'.format(dream_name,i))\n",
    "        \n",
    "        # This impacts how quich the zoom is \n",
    "        x_trim = 8\n",
    "        y_trim = 4\n",
    "        img_result = img_result[y_trim:y_size-y_trim, x_trim:x_size-x_trim]\n",
    "        img_result = cv2.resize(img_result,(x_size, y_size))\n",
    "        \n",
    "        # Dimmer function\n",
    "        img_result[:,:,0] += 2 # red\n",
    "        img_result[:,:,1] += 2 # green\n",
    "        img_result[:,:,2] += 4 # blue\n",
    "        \n",
    "        img_result = np.clip(img_result, 0.0, 255.0)\n",
    "        img_result = img_result.astype(np.uint8)\n",
    "        img_result = recursive_optimize(layer_tensor=layer_tensor, image=img_result,\n",
    "                         # how clear is the dream vs original image        \n",
    "                         num_iterations=10, step_size=1.0, rescale_factor=0.5,\n",
    "                         # How many \"passes\" over the data. More passes, the more granular the gradients will be.\n",
    "                         num_repeats=1, blend=0.2)\n",
    "\n",
    "        img_result = np.clip(img_result, 0.0, 255.0)\n",
    "        img_result = img_result.astype(np.uint8)\n",
    "        result = PIL.Image.fromarray(img_result, mode='RGB')\n",
    "        result.save('dream/{}/image_{}.jpg'.format(dream_name,i+1))\n",
    "        created_count += 1\n",
    "        if created_count > max_count:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a short video of images generated in previous step (using opencv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "dream_name = \"northern_light\"\n",
    "dream_path = \"dream/{}\".format(dream_name)\n",
    "fps = 15\n",
    "capSize = (756,473) # size of the source video\n",
    "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "out = cv2.VideoWriter('DeepDream.mov', fourcc, fps, capSize)\n",
    "\n",
    "for i in range(1234567):\n",
    "    if os.path.isfile('dream/{}/image_{}.jpg'.format(dream_name,i+1)):\n",
    "        print('{} already exists, continueing along...'.format(i+1))\n",
    "    else:\n",
    "        dream_length = i\n",
    "        break\n",
    "\n",
    "for i in range(dream_length):\n",
    "    img_path = os.path.join(dream_path, \"image_{}.jpg\".format(i))\n",
    "    frame = cv2.imread(img_path)\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
